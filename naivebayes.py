# -*- coding: utf-8 -*-
"""GNB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zi0L4nICBclHSqbwwa0bCyiAZF5ySR3W
"""

import numpy as np
from ucimlrepo import fetch_ucirepo
import matplotlib.pyplot as plt

# fetch dataset
breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)

# data (as pandas dataframes)
X = breast_cancer_wisconsin_diagnostic.data.features
y = breast_cancer_wisconsin_diagnostic.data.targets

X = X.to_numpy()
y = y.to_numpy()

numeric_y = np.array([1 if val == 'B' else 0 for val in y])


#custom splitting
def custom_train_test_split(X, y, test_size=0.29):

    np.random.seed(2)

    # Shuffle the indices
    indices = np.random.permutation(len(X))
    split_index = int((1 - test_size) * len(X))

    train_indices = indices[:split_index]
    test_indices = indices[split_index:]

    X_train, X_test = X[train_indices], X[test_indices]
    y_train, y_test = y[train_indices], y[test_indices]

    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = custom_train_test_split(X, numeric_y)

def fit_naive_bayes(X, y):
    n_samples, n_features = X.shape
    classes = np.unique(y)
    n_classes = len(classes)

    mean = np.zeros((n_classes, n_features))
    var = np.zeros((n_classes, n_features))
    priors = np.zeros(n_classes)

    for idx, c in enumerate(classes):
        X_c = X[y == c]
        mean[idx, :] = X_c.mean()
        var[idx, :] = X_c.var()
        priors[idx] = X_c.shape[0] / float(n_samples)

    return classes, mean, var, priors

def predict_naive_bayes(X, classes, mean, var, priors, alpha=1):
    def pdf(class_idx, x, alpha=1, epsilon=1e-9):
        class_mean = mean[class_idx]
        class_var = var[class_idx] + alpha  # Add pseudocount (alpha) to the variance
        class_var = np.maximum(class_var, epsilon)  # Ensure variance is not zero
        numerator = np.exp(- (x - class_mean) ** 2 / (2 * class_var))
        denominator = np.sqrt(2 * np.pi * class_var)
        return numerator / denominator

    def predict(x):
        posteriors = []
        for idx, c in enumerate(classes):
            prior = np.log(priors[idx])
            class_conditional = np.sum(np.log(pdf(idx, x)))
            posterior = prior + class_conditional
            posteriors.append(posterior)

        return classes[np.argmax(posteriors)]

    predictions = [predict(x) for x in X]
    return np.array(predictions)


classes, mean, var, priors = fit_naive_bayes(X_train, y_train)
predictions = predict_naive_bayes(X_train, classes, mean, var, priors)
train_accuracy = np.mean(predictions == y_train)


predictions = predict_naive_bayes(X_test, classes, mean, var, priors)
test_accuracy = np.mean(predictions == y_test)

print(train_accuracy, test_accuracy)

