# -*- coding: utf-8 -*-
"""CMPE462 Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aWFI5G8in9kscv1Bh_mznbiwA-5O6Z0v
"""

## importing libraries
import numpy as np
import matplotlib.pyplot as plt


##loading the data
data_large = np.load('data_large.npy')
data_small= np.load('data_small.npy')
label_large = np.load('label_large.npy')
label_small = np.load('label_small.npy')

##creating the weight vectors and adding bias term
w_1 = np.zeros(2)
w_2 = np.random.uniform(-1,1,2)
bias = np.random.uniform(-1,1,1)
w_1 = np.insert(w_1, [0], bias)
w_2 = np.insert(w_2, [0], bias)

##defining the sign function
def sign(input_x):
    if input_x > 0 :
        return 1
    if input_x < 0:
        return -1
    else:
        return 'no decision'

  ##defining perceptron algorithm
def PLA(input_x,output_y,w):
    iteration_number = 0 ##setting the iteration number to 0
    check = False  ## setting the check to false to control the while loop
    while (not check):
        misclassified = 0
        for i,feature in enumerate(input_x): ##iterating over the data points
            signof_x = sign(np.dot(w,feature))
            if signof_x != sign(output_y[i]):  ##finding misclassification
                w = w + output_y[i] * feature
                misclassified += 1
        iteration_number += 1
        if misclassified == 0:  ##if no misclassification, terminating the while loop
            check = True

    return (w, iteration_number)
##getting the results
result_large = PLA(data_large, label_large, w_1)
result_small = PLA(data_small, label_small, w_1)
result_small_w_2 = PLA(data_small, label_small, w_2)

print('Weights for the large dataset are', result_large[0], ',', 'iteration number is', result_large[1])
print('Weights for the small dataset are', result_small[0], ',', 'iteration number is', result_small[1])
print('Weights for the small dataset with different initial weights are', result_small_w_2[0], ',', 'iteration number is', result_small_w_2[1])


##plotting the decision boundary for the large and small dataset with the unchanged initial weights
def decision_boundary(input_x, output_y, w, ax=None, title = None):
    x_1 = input_x[:, 1]
    x_2 = input_x[:, 2]
    label = output_y
    if ax is None:
        fig, ax = plt.subplots()
    ax.scatter(x_1[label == 1], x_2[label == 1], color='blue', label='Class 1', marker='o', s=4)
    ax.scatter(x_1[label == -1], x_2[label == -1], color='red', label='Class -1', marker='^', s=4)
    x_1_min, x_1_max = min(x_1), max(x_1)
    x_2_min, x_2_max = min(x_2), max(x_2)
    boundary_x_1 = np.array([x_1_min, x_1_max])
    boundary_x_2 = -(w[0] + w[1] * boundary_x_1) / w[2]
    ax.plot(boundary_x_1, boundary_x_2, color='green', label='Decision Boundary')
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.set_title(None)
    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
    ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.grid(True)
    if title:
        ax.set_title(title)


fig, axs = plt.subplots(1, 2, figsize=(15, 5))

decision_boundary(data_large, label_large, result_large[0], ax=axs[0], title = 'Large Data Set')
decision_boundary(data_small, label_small, result_small[0], ax=axs[1], title = 'Small Data Set')
plt.subplots_adjust(wspace=0.5)
for ax in axs[:1]:
    ax.legend().set_visible(False)
plt.show()
