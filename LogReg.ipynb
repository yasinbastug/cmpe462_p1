{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/osmanyasinbastug/miniconda3/envs/mlpro/lib/python3.10/site-packages (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 545, 'name': 'Rice (Cammeo and Osmancik)', 'repository_url': 'https://archive.ics.uci.edu/dataset/545/rice+cammeo+and+osmancik', 'data_url': 'https://archive.ics.uci.edu/static/public/545/data.csv', 'abstract': \"A total of 3810 rice grain's images were taken for the two species, processed and feature inferences were made. 7 morphological features were obtained for each grain of rice.\", 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 3810, 'num_features': 7, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2019, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C5MW4Z', 'creators': [], 'intro_paper': {'title': 'Classification of Rice Varieties Using Artificial Intelligence Methods', 'authors': 'Ilkay Cinar, M. Koklu', 'published_in': 'International Journal of Intelligent Systems and Applications in Engineering', 'year': 2019, 'url': 'https://www.semanticscholar.org/paper/4e508bb906c8fdc04ead6f20bd8918fcb3605d1c', 'doi': '10.18201/ijisae.2019355381'}, 'additional_info': {'summary': \"Among  the certified rice grown in TURKEY,  the  Osmancik species, which has a large planting area since 1997 and the Cammeo species grown since 2014 have been selected for the study.  When  looking  at  the  general  characteristics  of  Osmancik species, they have a wide, long, glassy and dull appearance.  When looking at the general characteristics of the Cammeo species, they have wide and long, glassy and dull in appearance.  A total of 3810 rice grain's images were taken for the two species, processed and feature inferences were made. 7 morphological features were obtained for each grain of rice. \", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1.) Area: Returns  the  number  of  pixels  within  the boundaries of the rice grain.\\r\\n2.) Perimeter: Calculates the circumference by calculating  the  distance  between  pixels around the boundaries of the rice grain.\\r\\n3.) Major Axis Length: The longest line that can be drawn on the rice  grain,  i.e.  the  main  axis  distance, gives.\\r\\n4.) Minor Axis Length: The shortest line that can be drawn on the rice  grain,  i.e.  the  small  axis  distance, gives.\\r\\n5.) Eccentricity: It measures how round the ellipse, which has  the  same  moments  as  the  rice  grain, is.\\r\\n6.) Convex Area: Returns  the  pixel  count  of  the  smallest convex shell of the region formed by the rice grain.\\r\\n7.) Extent: Returns the ratio of the regionformed by the rice grain to the bounding box pixels.\\r\\n8.) Class: Cammeo and Osmancik rices', 'citation': None}}\n",
      "                name     role        type demographic  \\\n",
      "0               Area  Feature     Integer        None   \n",
      "1          Perimeter  Feature  Continuous        None   \n",
      "2  Major_Axis_Length  Feature  Continuous        None   \n",
      "3  Minor_Axis_Length  Feature  Continuous        None   \n",
      "4       Eccentricity  Feature  Continuous        None   \n",
      "5        Convex_Area  Feature     Integer        None   \n",
      "6             Extent  Feature  Continuous        None   \n",
      "7              Class   Target      Binary        None   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0  Returns the number of pixels within the bounda...    px             no  \n",
      "1  Calculates the circumference by calculating th...    px             no  \n",
      "2  The longest line that can be drawn on the rice...  None             no  \n",
      "3  The shortest line that can be drawn on the ric...  None             no  \n",
      "4  It measures how round the ellipse, which has t...  None             no  \n",
      "5  Returns the pixel count of the smallest convex...  None             no  \n",
      "6  Returns the ratio of the region formed by the ...  None             no  \n",
      "7                                Cammeo and Osmancik  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = rice_cammeo_and_osmancik.data.features \n",
    "y = rice_cammeo_and_osmancik.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(rice_cammeo_and_osmancik.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(rice_cammeo_and_osmancik.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Extent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15231</td>\n",
       "      <td>525.578979</td>\n",
       "      <td>229.749878</td>\n",
       "      <td>85.093788</td>\n",
       "      <td>0.928882</td>\n",
       "      <td>15617</td>\n",
       "      <td>0.572896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14656</td>\n",
       "      <td>494.311005</td>\n",
       "      <td>206.020065</td>\n",
       "      <td>91.730972</td>\n",
       "      <td>0.895405</td>\n",
       "      <td>15072</td>\n",
       "      <td>0.615436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14634</td>\n",
       "      <td>501.122009</td>\n",
       "      <td>214.106781</td>\n",
       "      <td>87.768288</td>\n",
       "      <td>0.912118</td>\n",
       "      <td>14954</td>\n",
       "      <td>0.693259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13176</td>\n",
       "      <td>458.342987</td>\n",
       "      <td>193.337387</td>\n",
       "      <td>87.448395</td>\n",
       "      <td>0.891861</td>\n",
       "      <td>13368</td>\n",
       "      <td>0.640669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14688</td>\n",
       "      <td>507.166992</td>\n",
       "      <td>211.743378</td>\n",
       "      <td>89.312454</td>\n",
       "      <td>0.906691</td>\n",
       "      <td>15262</td>\n",
       "      <td>0.646024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area   Perimeter  Major_Axis_Length  Minor_Axis_Length  Eccentricity  \\\n",
       "0  15231  525.578979         229.749878          85.093788      0.928882   \n",
       "1  14656  494.311005         206.020065          91.730972      0.895405   \n",
       "2  14634  501.122009         214.106781          87.768288      0.912118   \n",
       "3  13176  458.342987         193.337387          87.448395      0.891861   \n",
       "4  14688  507.166992         211.743378          89.312454      0.906691   \n",
       "\n",
       "   Convex_Area    Extent  \n",
       "0        15617  0.572896  \n",
       "1        15072  0.615436  \n",
       "2        14954  0.693259  \n",
       "3        13368  0.640669  \n",
       "4        15262  0.646024  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cammeo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cammeo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cammeo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cammeo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cammeo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class\n",
       "0  Cammeo\n",
       "1  Cammeo\n",
       "2  Cammeo\n",
       "3  Cammeo\n",
       "4  Cammeo"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rice = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3810 entries, 0 to 3809\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Area               3810 non-null   int64  \n",
      " 1   Perimeter          3810 non-null   float64\n",
      " 2   Major_Axis_Length  3810 non-null   float64\n",
      " 3   Minor_Axis_Length  3810 non-null   float64\n",
      " 4   Eccentricity       3810 non-null   float64\n",
      " 5   Convex_Area        3810 non-null   int64  \n",
      " 6   Extent             3810 non-null   float64\n",
      " 7   Class              3810 non-null   object \n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 238.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    Area   Perimeter  Major_Axis_Length  Minor_Axis_Length  Eccentricity  \\\n",
       " 0  15231  525.578979         229.749878          85.093788      0.928882   \n",
       " 1  14656  494.311005         206.020065          91.730972      0.895405   \n",
       " 2  14634  501.122009         214.106781          87.768288      0.912118   \n",
       " 3  13176  458.342987         193.337387          87.448395      0.891861   \n",
       " 4  14688  507.166992         211.743378          89.312454      0.906691   \n",
       " \n",
       "    Convex_Area    Extent   Class  \n",
       " 0        15617  0.572896  Cammeo  \n",
       " 1        15072  0.615436  Cammeo  \n",
       " 2        14954  0.693259  Cammeo  \n",
       " 3        13368  0.640669  Cammeo  \n",
       " 4        15262  0.646024  Cammeo  ,\n",
       "                Area    Perimeter  Major_Axis_Length  Minor_Axis_Length  \\\n",
       " count   3810.000000  3810.000000        3810.000000        3810.000000   \n",
       " mean   12667.727559   454.239180         188.776222          86.313750   \n",
       " std     1732.367706    35.597081          17.448679           5.729817   \n",
       " min     7551.000000   359.100006         145.264465          59.532406   \n",
       " 25%    11370.500000   426.144752         174.353855          82.731695   \n",
       " 50%    12421.500000   448.852493         185.810059          86.434647   \n",
       " 75%    13950.000000   483.683746         203.550438          90.143677   \n",
       " max    18913.000000   548.445984         239.010498         107.542450   \n",
       " \n",
       "        Eccentricity   Convex_Area       Extent  \n",
       " count   3810.000000   3810.000000  3810.000000  \n",
       " mean       0.886871  12952.496850     0.661934  \n",
       " std        0.020818   1776.972042     0.077239  \n",
       " min        0.777233   7723.000000     0.497413  \n",
       " 25%        0.872402  11626.250000     0.598862  \n",
       " 50%        0.889050  12706.500000     0.645361  \n",
       " 75%        0.902588  14284.000000     0.726562  \n",
       " max        0.948007  19099.000000     0.861050  ,\n",
       " None)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset to understand its structure\n",
    "data_rice.head(), data_rice.describe(), data_rice.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cammeo', 'Osmancik'], dtype=object)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the distinct values in the target column\n",
    "data_rice['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize the data\n",
    "for column in data_rice.select_dtypes(include=['number']).columns:  # This ensures only numeric columns are selected (not the target column)\n",
    "    min_col = data_rice[column].min()\n",
    "    max_col = data_rice[column].max()\n",
    "\n",
    "    # Avoid division by zero in case max_col equals min_col\n",
    "    if max_col != min_col:\n",
    "        data_rice[column] = (data_rice[column] - min_col) / (max_col - min_col)\n",
    "    else:\n",
    "        data_rice[column] = 0  # Assign 0 to all rows if max_col equals min_col \n",
    "\n",
    "# Replace the Class column with 0 and 1\n",
    "data_rice['Class'] = data_rice['Class'].replace('Cammeo', 0)\n",
    "data_rice['Class'] = data_rice['Class'].replace('Osmancik', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.675937</td>\n",
       "      <td>0.879232</td>\n",
       "      <td>0.901216</td>\n",
       "      <td>0.532417</td>\n",
       "      <td>0.888011</td>\n",
       "      <td>0.693917</td>\n",
       "      <td>0.207577</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.625330</td>\n",
       "      <td>0.714095</td>\n",
       "      <td>0.648087</td>\n",
       "      <td>0.670663</td>\n",
       "      <td>0.691980</td>\n",
       "      <td>0.646009</td>\n",
       "      <td>0.324564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.623394</td>\n",
       "      <td>0.750066</td>\n",
       "      <td>0.734349</td>\n",
       "      <td>0.588124</td>\n",
       "      <td>0.789846</td>\n",
       "      <td>0.635636</td>\n",
       "      <td>0.538576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495071</td>\n",
       "      <td>0.524136</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>0.581461</td>\n",
       "      <td>0.671227</td>\n",
       "      <td>0.496220</td>\n",
       "      <td>0.393954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.628146</td>\n",
       "      <td>0.781992</td>\n",
       "      <td>0.709138</td>\n",
       "      <td>0.620288</td>\n",
       "      <td>0.758067</td>\n",
       "      <td>0.662711</td>\n",
       "      <td>0.408680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Eccentricity  \\\n",
       "0  0.675937   0.879232           0.901216           0.532417      0.888011   \n",
       "1  0.625330   0.714095           0.648087           0.670663      0.691980   \n",
       "2  0.623394   0.750066           0.734349           0.588124      0.789846   \n",
       "3  0.495071   0.524136           0.512800           0.581461      0.671227   \n",
       "4  0.628146   0.781992           0.709138           0.620288      0.758067   \n",
       "\n",
       "   Convex_Area    Extent  Class  \n",
       "0     0.693917  0.207577      0  \n",
       "1     0.646009  0.324564      0  \n",
       "2     0.635636  0.538576      0  \n",
       "3     0.496220  0.393954      0  \n",
       "4     0.662711  0.408680      0  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_rice.drop('Class', axis=1) \n",
    "y = data_rice['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Extent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.675937</td>\n",
       "      <td>0.879232</td>\n",
       "      <td>0.901216</td>\n",
       "      <td>0.532417</td>\n",
       "      <td>0.888011</td>\n",
       "      <td>0.693917</td>\n",
       "      <td>0.207577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.625330</td>\n",
       "      <td>0.714095</td>\n",
       "      <td>0.648087</td>\n",
       "      <td>0.670663</td>\n",
       "      <td>0.691980</td>\n",
       "      <td>0.646009</td>\n",
       "      <td>0.324564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.623394</td>\n",
       "      <td>0.750066</td>\n",
       "      <td>0.734349</td>\n",
       "      <td>0.588124</td>\n",
       "      <td>0.789846</td>\n",
       "      <td>0.635636</td>\n",
       "      <td>0.538576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495071</td>\n",
       "      <td>0.524136</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>0.581461</td>\n",
       "      <td>0.671227</td>\n",
       "      <td>0.496220</td>\n",
       "      <td>0.393954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.628146</td>\n",
       "      <td>0.781992</td>\n",
       "      <td>0.709138</td>\n",
       "      <td>0.620288</td>\n",
       "      <td>0.758067</td>\n",
       "      <td>0.662711</td>\n",
       "      <td>0.408680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Eccentricity  \\\n",
       "0  0.675937   0.879232           0.901216           0.532417      0.888011   \n",
       "1  0.625330   0.714095           0.648087           0.670663      0.691980   \n",
       "2  0.623394   0.750066           0.734349           0.588124      0.789846   \n",
       "3  0.495071   0.524136           0.512800           0.581461      0.671227   \n",
       "4  0.628146   0.781992           0.709138           0.620288      0.758067   \n",
       "\n",
       "   Convex_Area    Extent  \n",
       "0     0.693917  0.207577  \n",
       "1     0.646009  0.324564  \n",
       "2     0.635636  0.538576  \n",
       "3     0.496220  0.393954  \n",
       "4     0.662711  0.408680  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62533005 0.71409491 0.64808716 0.67066313 0.69197987 0.64600914\n",
      " 0.32456423]\n"
     ]
    }
   ],
   "source": [
    "print(X.to_numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_rice):\n",
    "    # data refreshing\n",
    "    shuffled_data = data_rice.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Define the split size for the training set\n",
    "    train_size = int(0.8 * len(shuffled_data))  # 80% of data for training, 20% for testing\n",
    "\n",
    "    # Split the data\n",
    "    train_data = shuffled_data[:train_size]\n",
    "    test_data = shuffled_data[train_size:]\n",
    "\n",
    "    X_train = train_data.drop('Class', axis=1)  # Replace 'target_column' with your actual target column name\n",
    "    y_train = train_data['Class']\n",
    "\n",
    "    X_test = test_data.drop('Class', axis=1)\n",
    "    y_test = test_data['Class']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def initialize_weights(dim):\n",
    "    w = np.zeros(dim)  # initialize the weights to zeros\n",
    "    b = 0.0 # initialize the bias to zero\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)  # Clip z to avoid extreme values that lead to overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "# epsilon to avoid division by zero\n",
    "\n",
    "def logloss(y_true, y_pred, epsilon=1e-8):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predictions to avoid log(0)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "def gradient_dw(x, y, w, b, alpha, N):  # negative gradient of the log-likelihood function w.r.t w\n",
    "  '''In this function, we will compute the gradient w.r.to w ''' \n",
    "  dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w*w)/N)   \n",
    "  return dw\n",
    "\n",
    "\n",
    "def gradient_db(x, y, w, b):  # negative gradient of the log-likelihood function w.r.t b\n",
    "  '''In this function, we will compute gradient w.r.to b ''' \n",
    "  db = y-sigmoid(np.dot(w.T,x)+b)\n",
    "  return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y, w, b):\n",
    "  y_pred = []\n",
    "  X = np.array(X)\n",
    "  for i in range(len(X)):\n",
    "    z = sigmoid(np.dot(w.T, X[i]) + b)\n",
    "    y_pred.append(z)\n",
    "  # turn y into a numpy array\n",
    "  \n",
    "  loss = logloss(y, y_pred)\n",
    "  return loss\n",
    "\n",
    "def predict(X, w, b):\n",
    "  y_pred = []\n",
    "  for i in range(len(X)):\n",
    "    if sigmoid(np.dot(w.T, X[i]) + b) > 0.5:\n",
    "      y_pred.append(1)\n",
    "    else:\n",
    "      y_pred.append(0)\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_sgd(X, y, w, b, alpha, lr, epochs):\n",
    "    N = len(X)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)  # Ensure y is also a numpy array\n",
    "    prev_loss = np.inf\n",
    "    epsilon = 1e-5\n",
    "    loss = logloss(y, 0, epsilon)\n",
    "    print(f'Initial Loss: {loss}')\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data\n",
    "        shuffle_index = np.random.permutation(N)\n",
    "        X_shuffled, y_shuffled = X[shuffle_index], y[shuffle_index]\n",
    "\n",
    "        epoch_losses = []  # Collect losses for each instance\n",
    "\n",
    "        for i in range(N):\n",
    "            dw = gradient_dw(X_shuffled[i], y_shuffled[i], w, b, alpha, N)\n",
    "            db = gradient_db(X_shuffled[i], y_shuffled[i], w, b)\n",
    "            w = w + lr * dw\n",
    "            b = b + lr * db\n",
    "            \n",
    "            # Predict on the shuffled instance and calculate loss\n",
    "            y_pred = sigmoid(np.dot(w.T, X_shuffled[i]) + b)\n",
    "            loss = logloss(y_shuffled[i], y_pred, epsilon)\n",
    "            # isnan\n",
    "\n",
    "            epoch_losses.append(loss)\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        avg_epoch_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_epoch_loss)\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_epoch_loss}')\n",
    "\n",
    "        # Early stopping (optional) based on average loss\n",
    "        if (epoch > 0 and avg_epoch_loss > prev_loss) or np.isnan(avg_epoch_loss): # If the average loss is nan\n",
    "            print(\"Stopping early due to increase in average loss.\")\n",
    "            break\n",
    "        prev_loss = avg_epoch_loss\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    return w, b, losses, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_gd(X, y, w, b, alpha, lr, epochs):\n",
    "    N = len(X)\n",
    "    X = np.array(X)\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Calculate the gradient for the whole dataset\n",
    "        dw = np.mean([gradient_dw(X[i], y[i], w, b, alpha, N) for i in range(N)], axis=0)\n",
    "        db = np.mean([gradient_db(X[i], y[i], w, b) for i in range(N)])\n",
    "\n",
    "        # Update weights and bias for the whole dataset\n",
    "        w = w + lr * dw\n",
    "        b = b + lr * db\n",
    "\n",
    "        # Calculate predictions and loss for the entire dataset\n",
    "        y_pred = sigmoid(np.dot(X, w) + b)\n",
    "        loss = logloss(y, y_pred)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Loss: {loss}')\n",
    "\n",
    "        # Stop if the loss is not decreasing\n",
    "        if epoch > 0 and loss >= prev_loss:\n",
    "            end_time = time.time()\n",
    "            training_time = end_time - start_time\n",
    "            print(\"Stopping early due to increase in loss.\")\n",
    "            break\n",
    "        prev_loss = loss\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    return w, b, losses, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = data_rice.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split size for the training set\n",
    "train_size = int(0.8 * len(shuffled_data))  # 80% of data for training, 20% for testing\n",
    "\n",
    "# Split the data\n",
    "train_data = shuffled_data[:train_size]\n",
    "test_data = shuffled_data[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('Class', axis=1)  # Replace 'target_column' with your actual target column name\n",
    "y_train = train_data['Class']\n",
    "\n",
    "X_test = test_data.drop('Class', axis=1)\n",
    "y_test = test_data['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3048, 7) (3048,) (762, 7) (762,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression trained with GD (alpha = 0), no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {}\n",
    "accuracy_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Extent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.430470</td>\n",
       "      <td>0.530431</td>\n",
       "      <td>0.450626</td>\n",
       "      <td>0.576023</td>\n",
       "      <td>0.632941</td>\n",
       "      <td>0.458685</td>\n",
       "      <td>0.247959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.427478</td>\n",
       "      <td>0.411490</td>\n",
       "      <td>0.367767</td>\n",
       "      <td>0.610231</td>\n",
       "      <td>0.539372</td>\n",
       "      <td>0.428534</td>\n",
       "      <td>0.383667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.467875</td>\n",
       "      <td>0.475215</td>\n",
       "      <td>0.388668</td>\n",
       "      <td>0.662542</td>\n",
       "      <td>0.510778</td>\n",
       "      <td>0.477233</td>\n",
       "      <td>0.417034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487502</td>\n",
       "      <td>0.601254</td>\n",
       "      <td>0.611622</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>0.787528</td>\n",
       "      <td>0.492968</td>\n",
       "      <td>0.764162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247140</td>\n",
       "      <td>0.266237</td>\n",
       "      <td>0.299463</td>\n",
       "      <td>0.361245</td>\n",
       "      <td>0.697063</td>\n",
       "      <td>0.244989</td>\n",
       "      <td>0.209481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Eccentricity  \\\n",
       "0  0.430470   0.530431           0.450626           0.576023      0.632941   \n",
       "1  0.427478   0.411490           0.367767           0.610231      0.539372   \n",
       "2  0.467875   0.475215           0.388668           0.662542      0.510778   \n",
       "3  0.487502   0.601254           0.611622           0.493600      0.787528   \n",
       "4  0.247140   0.266237           0.299463           0.361245      0.697063   \n",
       "\n",
       "   Convex_Area    Extent  \n",
       "0     0.458685  0.247959  \n",
       "1     0.428534  0.383667  \n",
       "2     0.477233  0.417034  \n",
       "3     0.492968  0.764162  \n",
       "4     0.244989  0.209481  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6814436551651445\n",
      "Epoch: 1, Loss: 0.6703320001731111\n",
      "Epoch: 2, Loss: 0.6596372831288482\n",
      "Epoch: 3, Loss: 0.6493310293083785\n",
      "Epoch: 4, Loss: 0.6393966913641388\n",
      "Epoch: 5, Loss: 0.629819046403142\n",
      "Epoch: 6, Loss: 0.6205833846598842\n",
      "Epoch: 7, Loss: 0.6116754648248881\n",
      "Epoch: 8, Loss: 0.6030815240850681\n",
      "Epoch: 9, Loss: 0.5947882875082382\n",
      "Epoch: 10, Loss: 0.5867829730658911\n",
      "Epoch: 11, Loss: 0.5790532925820591\n",
      "Epoch: 12, Loss: 0.5715874491624622\n",
      "Epoch: 13, Loss: 0.5643741316380754\n",
      "Epoch: 14, Loss: 0.5574025065146166\n",
      "Epoch: 15, Loss: 0.550662207874187\n",
      "Epoch: 16, Loss: 0.5441433256297256\n",
      "Epoch: 17, Loss: 0.5378363924882511\n",
      "Epoch: 18, Loss: 0.5317323699359777\n",
      "Epoch: 19, Loss: 0.5258226335179568\n",
      "Epoch: 20, Loss: 0.5200989576473871\n",
      "Epoch: 21, Loss: 0.5145535001453646\n",
      "Epoch: 22, Loss: 0.5091787866807369\n",
      "Epoch: 23, Loss: 0.503967695251849\n",
      "Epoch: 24, Loss: 0.4989134408272171\n",
      "Epoch: 25, Loss: 0.4940095602403904\n",
      "Epoch: 26, Loss: 0.4892498974152413\n",
      "Epoch: 27, Loss: 0.4846285889814515\n",
      "Epoch: 28, Loss: 0.48014005032579715\n",
      "Epoch: 29, Loss: 0.47577896211274534\n",
      "Epoch: 30, Loss: 0.4715402572976357\n",
      "Epoch: 31, Loss: 0.4674191086471233\n",
      "Epoch: 32, Loss: 0.46341091677438606\n",
      "Epoch: 33, Loss: 0.45951129869069346\n",
      "Epoch: 34, Loss: 0.45571607687010307\n",
      "Epoch: 35, Loss: 0.45202126882015936\n",
      "Epoch: 36, Loss: 0.44842307714837765\n",
      "Epoch: 37, Loss: 0.4449178801118885\n",
      "Epoch: 38, Loss: 0.4415022226357814\n",
      "Epoch: 39, Loss: 0.4381728077843483\n",
      "Epoch: 40, Loss: 0.4349264886684779\n",
      "Epoch: 41, Loss: 0.43176026077185986\n",
      "Epoch: 42, Loss: 0.4286712546783262\n",
      "Epoch: 43, Loss: 0.4256567291825647\n",
      "Epoch: 44, Loss: 0.4227140647665281\n",
      "Epoch: 45, Loss: 0.4198407574240927\n",
      "Epoch: 46, Loss: 0.4170344128168753\n",
      "Epoch: 47, Loss: 0.41429274074455696\n",
      "Epoch: 48, Loss: 0.41161354991356824\n",
      "Epoch: 49, Loss: 0.4089947429885495\n",
      "Epoch: 50, Loss: 0.40643431191159235\n",
      "Epoch: 51, Loss: 0.40393033347487844\n",
      "Epoch: 52, Loss: 0.40148096513296044\n",
      "Epoch: 53, Loss: 0.39908444104155266\n",
      "Epoch: 54, Loss: 0.39673906831032707\n",
      "Epoch: 55, Loss: 0.39444322345782307\n",
      "Epoch: 56, Loss: 0.39219534905718023\n",
      "Epoch: 57, Loss: 0.38999395056199077\n",
      "Epoch: 58, Loss: 0.38783759330213474\n",
      "Epoch: 59, Loss: 0.38572489964000595\n",
      "Epoch: 60, Loss: 0.3836545462780627\n",
      "Epoch: 61, Loss: 0.3816252617091415\n",
      "Epoch: 62, Loss: 0.3796358238014481\n",
      "Epoch: 63, Loss: 0.37768505751060266\n",
      "Epoch: 64, Loss: 0.37577183271154685\n",
      "Epoch: 65, Loss: 0.3738950621435358\n",
      "Epoch: 66, Loss: 0.3720536994618299\n",
      "Epoch: 67, Loss: 0.3702467373900704\n",
      "Epoch: 68, Loss: 0.3684732059676737\n",
      "Epoch: 69, Loss: 0.36673217088691135\n",
      "Epoch: 70, Loss: 0.3650227319146547\n",
      "Epoch: 71, Loss: 0.3633440213940559\n",
      "Epoch: 72, Loss: 0.361695202821718\n",
      "Epoch: 73, Loss: 0.36007546949616387\n",
      "Epoch: 74, Loss: 0.35848404323366523\n",
      "Epoch: 75, Loss: 0.3569201731477189\n",
      "Epoch: 76, Loss: 0.3553831344886791\n",
      "Epoch: 77, Loss: 0.35387222754025843\n",
      "Epoch: 78, Loss: 0.3523867765698008\n",
      "Epoch: 79, Loss: 0.3509261288294134\n",
      "Epoch: 80, Loss: 0.3494896536052125\n",
      "Epoch: 81, Loss: 0.3480767413120993\n",
      "Epoch: 82, Loss: 0.34668680263162893\n",
      "Epoch: 83, Loss: 0.34531926769068183\n",
      "Epoch: 84, Loss: 0.34397358527877414\n",
      "Epoch: 85, Loss: 0.3426492221019699\n",
      "Epoch: 86, Loss: 0.3413456620714766\n",
      "Epoch: 87, Loss: 0.3400624056251121\n",
      "Epoch: 88, Loss: 0.3387989690799357\n",
      "Epoch: 89, Loss: 0.3375548840144336\n",
      "Epoch: 90, Loss: 0.33632969667874\n",
      "Epoch: 91, Loss: 0.3351229674314566\n",
      "Epoch: 92, Loss: 0.33393427020172234\n",
      "Epoch: 93, Loss: 0.33276319197525045\n",
      "Epoch: 94, Loss: 0.33160933230312967\n",
      "Epoch: 95, Loss: 0.3304723028322476\n",
      "Epoch: 96, Loss: 0.3293517268562602\n",
      "Epoch: 97, Loss: 0.328247238886089\n",
      "Epoch: 98, Loss: 0.32715848423898414\n",
      "Epoch: 99, Loss: 0.32608511864524287\n",
      "Epoch: 100, Loss: 0.3250268078717229\n",
      "Epoch: 101, Loss: 0.3239832273613369\n",
      "Epoch: 102, Loss: 0.322954061887757\n",
      "Epoch: 103, Loss: 0.32193900522459995\n",
      "Epoch: 104, Loss: 0.32093775982840267\n",
      "Epoch: 105, Loss: 0.3199500365347339\n",
      "Epoch: 106, Loss: 0.31897555426682195\n",
      "Epoch: 107, Loss: 0.31801403975611175\n",
      "Epoch: 108, Loss: 0.3170652272741948\n",
      "Epoch: 109, Loss: 0.3161288583755837\n",
      "Epoch: 110, Loss: 0.3152046816508312\n",
      "Epoch: 111, Loss: 0.31429245248951926\n",
      "Epoch: 112, Loss: 0.3133919328526664\n",
      "Epoch: 113, Loss: 0.312502891054128\n",
      "Epoch: 114, Loss: 0.31162510155058115\n",
      "Epoch: 115, Loss: 0.31075834473971037\n",
      "Epoch: 116, Loss: 0.3099024067662273\n",
      "Epoch: 117, Loss: 0.309057079335376\n",
      "Epoch: 118, Loss: 0.30822215953359455\n",
      "Epoch: 119, Loss: 0.307397449656017\n",
      "Epoch: 120, Loss: 0.3065827570405184\n",
      "Epoch: 121, Loss: 0.3057778939080169\n",
      "Epoch: 122, Loss: 0.30498267720876404\n",
      "Epoch: 123, Loss: 0.3041969284743648\n",
      "Epoch: 124, Loss: 0.30342047367528296\n",
      "Epoch: 125, Loss: 0.3026531430835983\n",
      "Epoch: 126, Loss: 0.3018947711407933\n",
      "Epoch: 127, Loss: 0.30114519633035847\n",
      "Epoch: 128, Loss: 0.30040426105501356\n",
      "Epoch: 129, Loss: 0.29967181151835354\n",
      "Epoch: 130, Loss: 0.2989476976107357\n",
      "Epoch: 131, Loss: 0.2982317727992332\n",
      "Epoch: 132, Loss: 0.29752389402148804\n",
      "Epoch: 133, Loss: 0.29682392158330617\n",
      "Epoch: 134, Loss: 0.2961317190598404\n",
      "Epoch: 135, Loss: 0.2954471532002192\n",
      "Epoch: 136, Loss: 0.29477009383548103\n",
      "Epoch: 137, Loss: 0.29410041378968355\n",
      "Epoch: 138, Loss: 0.29343798879406113\n",
      "Epoch: 139, Loss: 0.2927826974041103\n",
      "Epoch: 140, Loss: 0.2921344209194884\n",
      "Epoch: 141, Loss: 0.29149304330661485\n",
      "Epoch: 142, Loss: 0.2908584511238708\n",
      "Epoch: 143, Loss: 0.29023053344929606\n",
      "Epoch: 144, Loss: 0.28960918181068646\n",
      "Epoch: 145, Loss: 0.2889942901180022\n",
      "Epoch: 146, Loss: 0.2883857545979958\n",
      "Epoch: 147, Loss: 0.2877834737309779\n",
      "Epoch: 148, Loss: 0.28718734818963976\n",
      "Epoch: 149, Loss: 0.28659728077985436\n",
      "Epoch: 150, Loss: 0.28601317638338347\n",
      "Epoch: 151, Loss: 0.2854349419024189\n",
      "Epoch: 152, Loss: 0.28486248620589055\n",
      "Epoch: 153, Loss: 0.2842957200774762\n",
      "Epoch: 154, Loss: 0.28373455616525095\n",
      "Epoch: 155, Loss: 0.28317890893291564\n",
      "Epoch: 156, Loss: 0.2826286946125486\n",
      "Epoch: 157, Loss: 0.2820838311588241\n",
      "Epoch: 158, Loss: 0.281544238204646\n",
      "Epoch: 159, Loss: 0.28100983701814564\n",
      "Epoch: 160, Loss: 0.280480550460995\n",
      "Epoch: 161, Loss: 0.27995630294798984\n",
      "Epoch: 162, Loss: 0.2794370204078565\n",
      "Epoch: 163, Loss: 0.2789226302452409\n",
      "Epoch: 164, Loss: 0.2784130613038371\n",
      "Epoch: 165, Loss: 0.27790824383061774\n",
      "Epoch: 166, Loss: 0.27740810944112587\n",
      "Epoch: 167, Loss: 0.2769125910857941\n",
      "Epoch: 168, Loss: 0.27642162301725404\n",
      "Epoch: 169, Loss: 0.27593514075860304\n",
      "Epoch: 170, Loss: 0.27545308107259653\n",
      "Epoch: 171, Loss: 0.2749753819317327\n",
      "Epoch: 172, Loss: 0.27450198248920177\n",
      "Epoch: 173, Loss: 0.2740328230506693\n",
      "Epoch: 174, Loss: 0.27356784504686676\n",
      "Epoch: 175, Loss: 0.27310699100696234\n",
      "Epoch: 176, Loss: 0.27265020453268535\n",
      "Epoch: 177, Loss: 0.27219743027318144\n",
      "Epoch: 178, Loss: 0.2717486139005727\n",
      "Epoch: 179, Loss: 0.27130370208620086\n",
      "Epoch: 180, Loss: 0.2708626424775312\n",
      "Epoch: 181, Loss: 0.2704253836756963\n",
      "Epoch: 182, Loss: 0.26999187521365714\n",
      "Epoch: 183, Loss: 0.26956206753496537\n",
      "Epoch: 184, Loss: 0.2691359119731039\n",
      "Epoch: 185, Loss: 0.2687133607313896\n",
      "Epoch: 186, Loss: 0.26829436686342056\n",
      "Epoch: 187, Loss: 0.26787888425404904\n",
      "Epoch: 188, Loss: 0.2674668676008657\n",
      "Epoch: 189, Loss: 0.26705827239617774\n",
      "Epoch: 190, Loss: 0.2666530549094663\n",
      "Epoch: 191, Loss: 0.2662511721703083\n",
      "Epoch: 192, Loss: 0.2658525819517483\n",
      "Epoch: 193, Loss: 0.2654572427541067\n",
      "Epoch: 194, Loss: 0.26506511378921044\n",
      "Epoch: 195, Loss: 0.2646761549650346\n",
      "Epoch: 196, Loss: 0.26429032687074144\n",
      "Epoch: 197, Loss: 0.263907590762105\n",
      "Epoch: 198, Loss: 0.26352790854731045\n",
      "Epoch: 199, Loss: 0.26315124277311563\n",
      "Epoch: 200, Loss: 0.2627775566113662\n",
      "Epoch: 201, Loss: 0.2624068138458513\n",
      "Epoch: 202, Loss: 0.2620389788594918\n",
      "Epoch: 203, Loss: 0.26167401662185047\n",
      "Epoch: 204, Loss: 0.2613118926769545\n",
      "Epoch: 205, Loss: 0.2609525731314219\n",
      "Epoch: 206, Loss: 0.26059602464288245\n",
      "Epoch: 207, Loss: 0.260242214408685\n",
      "Epoch: 208, Loss: 0.2598911101548826\n",
      "Epoch: 209, Loss: 0.2595426801254877\n",
      "Epoch: 210, Loss: 0.25919689307199006\n",
      "Epoch: 211, Loss: 0.2588537182431291\n",
      "Epoch: 212, Loss: 0.2585131253749138\n",
      "Epoch: 213, Loss: 0.25817508468088407\n",
      "Epoch: 214, Loss: 0.2578395668426054\n",
      "Epoch: 215, Loss: 0.25750654300039094\n",
      "Epoch: 216, Loss: 0.257175984744245\n",
      "Epoch: 217, Loss: 0.25684786410502125\n",
      "Epoch: 218, Loss: 0.25652215354579005\n",
      "Epoch: 219, Loss: 0.2561988259534088\n",
      "Epoch: 220, Loss: 0.2558778546302904\n",
      "Epoch: 221, Loss: 0.2555592132863634\n",
      "Epoch: 222, Loss: 0.25524287603121987\n",
      "Epoch: 223, Loss: 0.2549288173664448\n",
      "Epoch: 224, Loss: 0.2546170121781227\n",
      "Epoch: 225, Loss: 0.2543074357295165\n",
      "Epoch: 226, Loss: 0.25400006365391425\n",
      "Epoch: 227, Loss: 0.253694871947639\n",
      "Epoch: 228, Loss: 0.25339183696321743\n",
      "Epoch: 229, Loss: 0.2530909354027034\n",
      "Epoch: 230, Loss: 0.2527921443111517\n",
      "Epoch: 231, Loss: 0.25249544107023936\n",
      "Epoch: 232, Loss: 0.25220080339202827\n",
      "Epoch: 233, Loss: 0.2519082093128682\n",
      "Epoch: 234, Loss: 0.2516176371874352\n",
      "Epoch: 235, Loss: 0.25132906568290053\n",
      "Epoch: 236, Loss: 0.2510424737732302\n",
      "Epoch: 237, Loss: 0.2507578407336082\n",
      "Epoch: 238, Loss: 0.2504751461349825\n",
      "Epoch: 239, Loss: 0.25019436983872967\n",
      "Epoch: 240, Loss: 0.24991549199143567\n",
      "Epoch: 241, Loss: 0.2496384930197893\n",
      "Epoch: 242, Loss: 0.24936335362558618\n",
      "Epoch: 243, Loss: 0.24909005478083956\n",
      "Epoch: 244, Loss: 0.2488185777229962\n",
      "Epoch: 245, Loss: 0.2485489039502543\n",
      "Epoch: 246, Loss: 0.24828101521698043\n",
      "Epoch: 247, Loss: 0.24801489352922418\n",
      "Epoch: 248, Loss: 0.24775052114032708\n",
      "Epoch: 249, Loss: 0.24748788054662396\n",
      "Epoch: 250, Loss: 0.24722695448323403\n",
      "Epoch: 251, Loss: 0.2469677259199405\n",
      "Epoch: 252, Loss: 0.24671017805715506\n",
      "Epoch: 253, Loss: 0.2464542943219669\n",
      "Epoch: 254, Loss: 0.24620005836427233\n",
      "Epoch: 255, Loss: 0.24594745405298507\n",
      "Epoch: 256, Loss: 0.24569646547232366\n",
      "Epoch: 257, Loss: 0.24544707691817452\n",
      "Epoch: 258, Loss: 0.24519927289452986\n",
      "Epoch: 259, Loss: 0.24495303810999722\n",
      "Epoch: 260, Loss: 0.2447083574743797\n",
      "Epoch: 261, Loss: 0.24446521609532568\n",
      "Epoch: 262, Loss: 0.24422359927504483\n",
      "Epoch: 263, Loss: 0.2439834925070908\n",
      "Epoch: 264, Loss: 0.24374488147320747\n",
      "Epoch: 265, Loss: 0.24350775204023822\n",
      "Epoch: 266, Loss: 0.2432720902570963\n",
      "Epoch: 267, Loss: 0.24303788235179513\n",
      "Epoch: 268, Loss: 0.2428051147285368\n",
      "Epoch: 269, Loss: 0.24257377396485802\n",
      "Epoch: 270, Loss: 0.24234384680883145\n",
      "Epoch: 271, Loss: 0.242115320176322\n",
      "Epoch: 272, Loss: 0.24188818114829594\n",
      "Epoch: 273, Loss: 0.24166241696818258\n",
      "Epoch: 274, Loss: 0.24143801503928625\n",
      "Epoch: 275, Loss: 0.24121496292224853\n",
      "Epoch: 276, Loss: 0.24099324833255903\n",
      "Epoch: 277, Loss: 0.24077285913811322\n",
      "Epoch: 278, Loss: 0.24055378335681762\n",
      "Epoch: 279, Loss: 0.24033600915423944\n",
      "Epoch: 280, Loss: 0.24011952484130156\n",
      "Epoch: 281, Loss: 0.23990431887202013\n",
      "Epoch: 282, Loss: 0.23969037984128552\n",
      "Epoch: 283, Loss: 0.23947769648268413\n",
      "Epoch: 284, Loss: 0.23926625766636134\n",
      "Epoch: 285, Loss: 0.23905605239692365\n",
      "Epoch: 286, Loss: 0.2388470698113806\n",
      "Epoch: 287, Loss: 0.23863929917712376\n",
      "Epoch: 288, Loss: 0.23843272988994382\n",
      "Epoch: 289, Loss: 0.2382273514720835\n",
      "Epoch: 290, Loss: 0.23802315357032647\n",
      "Epoch: 291, Loss: 0.23782012595412105\n",
      "Epoch: 292, Loss: 0.23761825851373786\n",
      "Epoch: 293, Loss: 0.23741754125846137\n",
      "Epoch: 294, Loss: 0.23721796431481365\n",
      "Epoch: 295, Loss: 0.2370195179248104\n",
      "Epoch: 296, Loss: 0.23682219244424835\n",
      "Epoch: 297, Loss: 0.23662597834102322\n",
      "Epoch: 298, Loss: 0.23643086619347778\n",
      "Epoch: 299, Loss: 0.23623684668877928\n",
      "Epoch: 300, Loss: 0.23604391062132585\n",
      "Epoch: 301, Loss: 0.23585204889118072\n",
      "Epoch: 302, Loss: 0.23566125250253472\n",
      "Epoch: 303, Loss: 0.23547151256219503\n",
      "Epoch: 304, Loss: 0.23528282027810118\n",
      "Epoch: 305, Loss: 0.23509516695786625\n",
      "Epoch: 306, Loss: 0.23490854400734415\n",
      "Epoch: 307, Loss: 0.2347229429292216\n",
      "Epoch: 308, Loss: 0.23453835532163417\n",
      "Epoch: 309, Loss: 0.23435477287680673\n",
      "Epoch: 310, Loss: 0.2341721873797169\n",
      "Epoch: 311, Loss: 0.23399059070678194\n",
      "Epoch: 312, Loss: 0.2338099748245673\n",
      "Epoch: 313, Loss: 0.23363033178851825\n",
      "Epoch: 314, Loss: 0.23345165374171215\n",
      "Epoch: 315, Loss: 0.23327393291363266\n",
      "Epoch: 316, Loss: 0.23309716161896407\n",
      "Epoch: 317, Loss: 0.2329213322564063\n",
      "Epoch: 318, Loss: 0.23274643730750993\n",
      "Epoch: 319, Loss: 0.23257246933553039\n",
      "Epoch: 320, Loss: 0.23239942098430166\n",
      "Epoch: 321, Loss: 0.23222728497712852\n",
      "Epoch: 322, Loss: 0.23205605411569744\n",
      "Epoch: 323, Loss: 0.23188572127900528\n",
      "Epoch: 324, Loss: 0.23171627942230585\n",
      "Epoch: 325, Loss: 0.2315477215760739\n",
      "Epoch: 326, Loss: 0.23138004084498578\n",
      "Epoch: 327, Loss: 0.23121323040691735\n",
      "Epoch: 328, Loss: 0.23104728351195788\n",
      "Epoch: 329, Loss: 0.2308821934814401\n",
      "Epoch: 330, Loss: 0.23071795370698578\n",
      "Epoch: 331, Loss: 0.23055455764956756\n",
      "Epoch: 332, Loss: 0.23039199883858502\n",
      "Epoch: 333, Loss: 0.23023027087095596\n",
      "Epoch: 334, Loss: 0.23006936741022266\n",
      "Epoch: 335, Loss: 0.22990928218567191\n",
      "Epoch: 336, Loss: 0.22975000899146944\n",
      "Epoch: 337, Loss: 0.2295915416858078\n",
      "Epoch: 338, Loss: 0.22943387419006836\n",
      "Epoch: 339, Loss: 0.22927700048799596\n",
      "Epoch: 340, Loss: 0.22912091462488715\n",
      "Epoch: 341, Loss: 0.22896561070679056\n",
      "Epoch: 342, Loss: 0.2288110828997207\n",
      "Epoch: 343, Loss: 0.22865732542888328\n",
      "Epoch: 344, Loss: 0.22850433257791294\n",
      "Epoch: 345, Loss: 0.22835209868812337\n",
      "Epoch: 346, Loss: 0.2282006181577679\n",
      "Epoch: 347, Loss: 0.2280498854413128\n",
      "Epoch: 348, Loss: 0.2278998950487212\n",
      "Epoch: 349, Loss: 0.22775064154474783\n",
      "Epoch: 350, Loss: 0.22760211954824522\n",
      "Epoch: 351, Loss: 0.2274543237314802\n",
      "Epoch: 352, Loss: 0.22730724881946063\n",
      "Epoch: 353, Loss: 0.2271608895892729\n",
      "Epoch: 354, Loss: 0.22701524086942923\n",
      "Epoch: 355, Loss: 0.22687029753922477\n",
      "Epoch: 356, Loss: 0.22672605452810468\n",
      "Epoch: 357, Loss: 0.22658250681504072\n",
      "Epoch: 358, Loss: 0.2264396494279168\n",
      "Epoch: 359, Loss: 0.22629747744292456\n",
      "Epoch: 360, Loss: 0.2261559859839674\n",
      "Epoch: 361, Loss: 0.22601517022207324\n",
      "Epoch: 362, Loss: 0.22587502537481693\n",
      "Epoch: 363, Loss: 0.2257355467057501\n",
      "Epoch: 364, Loss: 0.22559672952384047\n",
      "Epoch: 365, Loss: 0.22545856918291857\n",
      "Epoch: 366, Loss: 0.22532106108113353\n",
      "Epoch: 367, Loss: 0.22518420066041595\n",
      "Epoch: 368, Loss: 0.2250479834059494\n",
      "Epoch: 369, Loss: 0.22491240484564903\n",
      "Epoch: 370, Loss: 0.22477746054964848\n",
      "Epoch: 371, Loss: 0.22464314612979355\n",
      "Epoch: 372, Loss: 0.2245094572391438\n",
      "Epoch: 373, Loss: 0.22437638957148084\n",
      "Epoch: 374, Loss: 0.22424393886082422\n",
      "Epoch: 375, Loss: 0.22411210088095398\n",
      "Epoch: 376, Loss: 0.2239808714449402\n",
      "Epoch: 377, Loss: 0.22385024640467915\n",
      "Epoch: 378, Loss: 0.22372022165043642\n",
      "Epoch: 379, Loss: 0.22359079311039592\n",
      "Epoch: 380, Loss: 0.2234619567502161\n",
      "Epoch: 381, Loss: 0.22333370857259174\n",
      "Epoch: 382, Loss: 0.22320604461682264\n",
      "Epoch: 383, Loss: 0.2230789609583875\n",
      "Epoch: 384, Loss: 0.2229524537085249\n",
      "Epoch: 385, Loss: 0.22282651901381917\n",
      "Epoch: 386, Loss: 0.2227011530557928\n",
      "Epoch: 387, Loss: 0.222576352050504\n",
      "Epoch: 388, Loss: 0.2224521122481504\n",
      "Epoch: 389, Loss: 0.22232842993267754\n",
      "Epoch: 390, Loss: 0.22220530142139383\n",
      "Epoch: 391, Loss: 0.22208272306458976\n",
      "Epoch: 392, Loss: 0.22196069124516318\n",
      "Epoch: 393, Loss: 0.22183920237824922\n",
      "Epoch: 394, Loss: 0.2217182529108555\n",
      "Epoch: 395, Loss: 0.2215978393215026\n",
      "Epoch: 396, Loss: 0.2214779581198689\n",
      "Epoch: 397, Loss: 0.2213586058464408\n",
      "Epoch: 398, Loss: 0.2212397790721673\n",
      "Epoch: 399, Loss: 0.2211214743981194\n",
      "Test loss: 0.24018124377664835\n",
      "Accuracy: 0.9186351706036745\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(data_rice)\n",
    "\n",
    "w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "alpha = 0\n",
    "lr = 1\n",
    "epochs = 400\n",
    "w, b, losses, training_time = train_batch_gd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "test_loss = test(X_test, y_test, w, b)\n",
    "print(f'Test loss: {test_loss}')\n",
    "loss_dict[\"GD\"] = (test_loss)\n",
    "\n",
    "\n",
    "# test accuracy\n",
    "y_pred = predict(X_test.to_numpy(), w, b)\n",
    "test_accuracy = np.mean(y_pred == y_test.to_numpy())\n",
    "print(f'Accuracy: {test_accuracy}')\n",
    "\n",
    "# training accuracy\n",
    "y_pred = predict(X_train.to_numpy(), w, b)\n",
    "training_accuracy = np.mean(y_pred == y_train.to_numpy())\n",
    "\n",
    "accuracy_dict[\"GD\"] = (training_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross validation to find the best L2 regularization parameter for the Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shuffled_data = data_rice.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def kfold_split(data, k):\n",
    "    data_split = []\n",
    "    fold_size = int(len(data) / k)\n",
    "    for i in range(k):\n",
    "        fold = data[i * fold_size: (i + 1) * fold_size]\n",
    "        data_split.append(fold)\n",
    "    return data_split\n",
    "\n",
    "k = 5\n",
    "data_folds = kfold_split(shuffled_data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762, 8)\n"
     ]
    }
   ],
   "source": [
    "print(data_folds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.430470</td>\n",
       "      <td>0.530431</td>\n",
       "      <td>0.450626</td>\n",
       "      <td>0.576023</td>\n",
       "      <td>0.632941</td>\n",
       "      <td>0.458685</td>\n",
       "      <td>0.247959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.427478</td>\n",
       "      <td>0.411490</td>\n",
       "      <td>0.367767</td>\n",
       "      <td>0.610231</td>\n",
       "      <td>0.539372</td>\n",
       "      <td>0.428534</td>\n",
       "      <td>0.383667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.467875</td>\n",
       "      <td>0.475215</td>\n",
       "      <td>0.388668</td>\n",
       "      <td>0.662542</td>\n",
       "      <td>0.510778</td>\n",
       "      <td>0.477233</td>\n",
       "      <td>0.417034</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487502</td>\n",
       "      <td>0.601254</td>\n",
       "      <td>0.611622</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>0.787528</td>\n",
       "      <td>0.492968</td>\n",
       "      <td>0.764162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247140</td>\n",
       "      <td>0.266237</td>\n",
       "      <td>0.299463</td>\n",
       "      <td>0.361245</td>\n",
       "      <td>0.697063</td>\n",
       "      <td>0.244989</td>\n",
       "      <td>0.209481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Eccentricity  \\\n",
       "0  0.430470   0.530431           0.450626           0.576023      0.632941   \n",
       "1  0.427478   0.411490           0.367767           0.610231      0.539372   \n",
       "2  0.467875   0.475215           0.388668           0.662542      0.510778   \n",
       "3  0.487502   0.601254           0.611622           0.493600      0.787528   \n",
       "4  0.247140   0.266237           0.299463           0.361245      0.697063   \n",
       "\n",
       "   Convex_Area    Extent  Class  \n",
       "0     0.458685  0.247959      0  \n",
       "1     0.428534  0.383667      1  \n",
       "2     0.477233  0.417034      1  \n",
       "3     0.492968  0.764162      0  \n",
       "4     0.244989  0.209481      1  "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folds[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 with alpha: 0.01\n",
      "(609, 7) (609,) (153, 7) (153,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.681832993144924\n",
      "Epoch: 1, Loss: 0.6710020069642377\n",
      "Epoch: 2, Loss: 0.6605587240575653\n",
      "Epoch: 3, Loss: 0.6504822323209581\n",
      "Epoch: 4, Loss: 0.6407577467530223\n",
      "Epoch: 5, Loss: 0.6313712973785289\n",
      "Epoch: 6, Loss: 0.6223093388906646\n",
      "Epoch: 7, Loss: 0.6135587384006584\n",
      "Epoch: 8, Loss: 0.6051067876435922\n",
      "Epoch: 9, Loss: 0.5969412126782917\n",
      "Epoch: 10, Loss: 0.5890501795198468\n",
      "Epoch: 11, Loss: 0.581422296077875\n",
      "Epoch: 12, Loss: 0.5740466108907406\n",
      "Epoch: 13, Loss: 0.5669126091230499\n",
      "Epoch: 14, Loss: 0.560010206258525\n",
      "Epoch: 15, Loss: 0.5533297398827679\n",
      "Epoch: 16, Loss: 0.5468619599121094\n",
      "Epoch: 17, Loss: 0.5405980175867797\n",
      "Epoch: 18, Loss: 0.5345294535098558\n",
      "Epoch: 19, Loss: 0.5286481849785304\n",
      "Epoch: 20, Loss: 0.5229464928215962\n",
      "Epoch: 21, Loss: 0.5174170079269589\n",
      "Epoch: 22, Loss: 0.5120526976155778\n",
      "Epoch: 23, Loss: 0.5068468519935502\n",
      "Epoch: 24, Loss: 0.5017930703919926\n",
      "Epoch: 25, Loss: 0.49688524798487516\n",
      "Epoch: 26, Loss: 0.4921175626578299\n",
      "Epoch: 27, Loss: 0.4874844621860404\n",
      "Epoch: 28, Loss: 0.4829806517664129\n",
      "Epoch: 29, Loss: 0.4786010819381594\n",
      "Epoch: 30, Loss: 0.47434093691647994\n",
      "Epoch: 31, Loss: 0.47019562335604964\n",
      "Epoch: 32, Loss: 0.46616075955431335\n",
      "Epoch: 33, Loss: 0.4622321650990219\n",
      "Epoch: 34, Loss: 0.4584058509598419\n",
      "Epoch: 35, Loss: 0.4546780100201335\n",
      "Epoch: 36, Loss: 0.451045008041971\n",
      "Epoch: 37, Loss: 0.4475033750550954\n",
      "Epoch: 38, Loss: 0.44404979715863185\n",
      "Epoch: 39, Loss: 0.44068110872300176\n",
      "Epoch: 40, Loss: 0.4373942849784339\n",
      "Epoch: 41, Loss: 0.43418643497577253\n",
      "Epoch: 42, Loss: 0.43105479490484194\n",
      "Epoch: 43, Loss: 0.4279967217553965\n",
      "Epoch: 44, Loss: 0.4250096873056443\n",
      "Epoch: 45, Loss: 0.42209127242342864\n",
      "Epoch: 46, Loss: 0.4192391616653644\n",
      "Epoch: 47, Loss: 0.41645113815953194\n",
      "Epoch: 48, Loss: 0.41372507875770514\n",
      "Epoch: 49, Loss: 0.4110589494435186\n",
      "Epoch: 50, Loss: 0.40845080098344566\n",
      "Epoch: 51, Loss: 0.4058987648079506\n",
      "Epoch: 52, Loss: 0.40340104911069097\n",
      "Epoch: 53, Loss: 0.40095593515416217\n",
      "Epoch: 54, Loss: 0.3985617737706985\n",
      "Epoch: 55, Loss: 0.3962169820482629\n",
      "Epoch: 56, Loss: 0.39392004019096544\n",
      "Epoch: 57, Loss: 0.39166948854475386\n",
      "Epoch: 58, Loss: 0.38946392477920155\n",
      "Epoch: 59, Loss: 0.38730200121679265\n",
      "Epoch: 60, Loss: 0.38518242230155564\n",
      "Epoch: 61, Loss: 0.3831039421993371\n",
      "Epoch: 62, Loss: 0.3810653625224214\n",
      "Epoch: 63, Loss: 0.3790655301716081\n",
      "Epoch: 64, Loss: 0.3771033352892366\n",
      "Epoch: 65, Loss: 0.3751777093170123\n",
      "Epoch: 66, Loss: 0.37328762315283665\n",
      "Epoch: 67, Loss: 0.37143208540116807\n",
      "Epoch: 68, Loss: 0.3696101407117542\n",
      "Epoch: 69, Loss: 0.3678208682018689\n",
      "Epoch: 70, Loss: 0.36606337995746746\n",
      "Epoch: 71, Loss: 0.3643368196089344\n",
      "Epoch: 72, Loss: 0.3626403609773488\n",
      "Epoch: 73, Loss: 0.3609732067874253\n",
      "Epoch: 74, Loss: 0.3593345874435101\n",
      "Epoch: 75, Loss: 0.3577237598652199\n",
      "Epoch: 76, Loss: 0.35614000637950805\n",
      "Epoch: 77, Loss: 0.35458263366612675\n",
      "Epoch: 78, Loss: 0.35305097175362826\n",
      "Epoch: 79, Loss: 0.35154437306321223\n",
      "Epoch: 80, Loss: 0.3500622114978803\n",
      "Epoch: 81, Loss: 0.3486038815745038\n",
      "Epoch: 82, Loss: 0.34716879759654684\n",
      "Epoch: 83, Loss: 0.34575639286531745\n",
      "Epoch: 84, Loss: 0.34436611892773583\n",
      "Epoch: 85, Loss: 0.3429974448587276\n",
      "Epoch: 86, Loss: 0.3416498565764528\n",
      "Epoch: 87, Loss: 0.3403228561886835\n",
      "Epoch: 88, Loss: 0.3390159613687381\n",
      "Epoch: 89, Loss: 0.33772870475946626\n",
      "Epoch: 90, Loss: 0.33646063340386795\n",
      "Epoch: 91, Loss: 0.3352113082010005\n",
      "Epoch: 92, Loss: 0.3339803033859108\n",
      "Epoch: 93, Loss: 0.3327672060323909\n",
      "Epoch: 94, Loss: 0.3315716155774281\n",
      "Epoch: 95, Loss: 0.3303931433662768\n",
      "Epoch: 96, Loss: 0.32923141221714136\n",
      "Epoch: 97, Loss: 0.32808605600451185\n",
      "Epoch: 98, Loss: 0.3269567192602465\n",
      "Epoch: 99, Loss: 0.32584305679154457\n",
      "Fold 2 with alpha: 0.1\n",
      "(609, 7) (609,) (153, 7) (153,)\n",
      "Epoch: 0, Loss: 0.6812966526897875\n",
      "Epoch: 1, Loss: 0.6700561574056334\n",
      "Epoch: 2, Loss: 0.6592359868855681\n",
      "Epoch: 3, Loss: 0.6488056876311198\n",
      "Epoch: 4, Loss: 0.6387485988044707\n",
      "Epoch: 5, Loss: 0.6290495946612029\n",
      "Epoch: 6, Loss: 0.6196940752806425\n",
      "Epoch: 7, Loss: 0.6106679002815152\n",
      "Epoch: 8, Loss: 0.6019573965066192\n",
      "Epoch: 9, Loss: 0.5935493671967257\n",
      "Epoch: 10, Loss: 0.5854310971790553\n",
      "Epoch: 11, Loss: 0.5775903541544443\n",
      "Epoch: 12, Loss: 0.5700153865879903\n",
      "Epoch: 13, Loss: 0.5626949187057193\n",
      "Epoch: 14, Loss: 0.555618143062322\n",
      "Epoch: 15, Loss: 0.5487747111035347\n",
      "Epoch: 16, Loss: 0.542154722104643\n",
      "Epoch: 17, Loss: 0.535748710825098\n",
      "Epoch: 18, Loss: 0.5295476341793021\n",
      "Epoch: 19, Loss: 0.5235428571858466\n",
      "Epoch: 20, Loss: 0.5177261384223321\n",
      "Epoch: 21, Loss: 0.5120896151805924\n",
      "Epoch: 22, Loss: 0.506625788487792\n",
      "Epoch: 23, Loss: 0.5013275081324755\n",
      "Epoch: 24, Loss: 0.49618795781111463\n",
      "Epoch: 25, Loss: 0.4912006404899159\n",
      "Epoch: 26, Loss: 0.4863593640584209\n",
      "Epoch: 27, Loss: 0.48165822733556\n",
      "Epoch: 28, Loss: 0.4770916064751226\n",
      "Epoch: 29, Loss: 0.472654141805841\n",
      "Epoch: 30, Loss: 0.4683407251312735\n",
      "Epoch: 31, Loss: 0.46414648750621273\n",
      "Epoch: 32, Loss: 0.46006678749924546\n",
      "Epoch: 33, Loss: 0.4560971999451954\n",
      "Epoch: 34, Loss: 0.45223350518632816\n",
      "Epoch: 35, Loss: 0.4484716787972429\n",
      "Epoch: 36, Loss: 0.4448078817852087\n",
      "Epoch: 37, Loss: 0.44123845125519046\n",
      "Epoch: 38, Loss: 0.4377598915268766\n",
      "Epoch: 39, Loss: 0.4343688656895569\n",
      "Epoch: 40, Loss: 0.43106218757964126\n",
      "Epoch: 41, Loss: 0.4278368141649021\n",
      "Epoch: 42, Loss: 0.42468983831907253\n",
      "Epoch: 43, Loss: 0.4216184819702391\n",
      "Epoch: 44, Loss: 0.4186200896064432\n",
      "Epoch: 45, Loss: 0.41569212212204426\n",
      "Epoch: 46, Loss: 0.4128321509886565\n",
      "Epoch: 47, Loss: 0.4100378527348209\n",
      "Epoch: 48, Loss: 0.407307003719008\n",
      "Epoch: 49, Loss: 0.40463747518102455\n",
      "Epoch: 50, Loss: 0.40202722855742407\n",
      "Epoch: 51, Loss: 0.3994743110470706\n",
      "Epoch: 52, Loss: 0.39697685141357314\n",
      "Epoch: 53, Loss: 0.39453305601188315\n",
      "Epoch: 54, Loss: 0.3921412050269249\n",
      "Epoch: 55, Loss: 0.3897996489127011\n",
      "Epoch: 56, Loss: 0.3875068050208775\n",
      "Epoch: 57, Loss: 0.3852611544084043\n",
      "Epoch: 58, Loss: 0.383061238814265\n",
      "Epoch: 59, Loss: 0.38090565779596186\n",
      "Epoch: 60, Loss: 0.37879306601684987\n",
      "Epoch: 61, Loss: 0.37672217067590846\n",
      "Epoch: 62, Loss: 0.37469172907200066\n",
      "Epoch: 63, Loss: 0.3727005462951103\n",
      "Epoch: 64, Loss: 0.3707474730374673\n",
      "Epoch: 65, Loss: 0.3688314035178676\n",
      "Epoch: 66, Loss: 0.36695127351287576\n",
      "Epoch: 67, Loss: 0.36510605848895716\n",
      "Epoch: 68, Loss: 0.3632947718299269\n",
      "Epoch: 69, Loss: 0.3615164631544241\n",
      "Epoch: 70, Loss: 0.3597702167184275\n",
      "Epoch: 71, Loss: 0.3580551498981141\n",
      "Epoch: 72, Loss: 0.3563704117486336\n",
      "Epoch: 73, Loss: 0.3547151816346303\n",
      "Epoch: 74, Loss: 0.3530886679285834\n",
      "Epoch: 75, Loss: 0.35149010677326437\n",
      "Epoch: 76, Loss: 0.3499187609048263\n",
      "Epoch: 77, Loss: 0.34837391853323946\n",
      "Epoch: 78, Loss: 0.34685489227697824\n",
      "Epoch: 79, Loss: 0.345361018149045\n",
      "Epoch: 80, Loss: 0.3438916545915805\n",
      "Epoch: 81, Loss: 0.34244618155647333\n",
      "Epoch: 82, Loss: 0.34102399962952584\n",
      "Epoch: 83, Loss: 0.3396245291958751\n",
      "Epoch: 84, Loss: 0.3382472096445\n",
      "Epoch: 85, Loss: 0.3368914986097682\n",
      "Epoch: 86, Loss: 0.33555687124809075\n",
      "Epoch: 87, Loss: 0.3342428195478674\n",
      "Epoch: 88, Loss: 0.3329488516710008\n",
      "Epoch: 89, Loss: 0.3316744913243609\n",
      "Epoch: 90, Loss: 0.33041927715966735\n",
      "Epoch: 91, Loss: 0.3291827622003464\n",
      "Epoch: 92, Loss: 0.32796451329399595\n",
      "Epoch: 93, Loss: 0.3267641105891711\n",
      "Epoch: 94, Loss: 0.3255811470352718\n",
      "Epoch: 95, Loss: 0.32441522790438126\n",
      "Epoch: 96, Loss: 0.32326597033396803\n",
      "Epoch: 97, Loss: 0.32213300288942237\n",
      "Epoch: 98, Loss: 0.32101596514545444\n",
      "Epoch: 99, Loss: 0.3199145072854343\n",
      "Fold 3 with alpha: 1\n",
      "(609, 7) (609,) (153, 7) (153,)\n",
      "Epoch: 0, Loss: 0.6794056313890194\n",
      "Epoch: 1, Loss: 0.6680638347578053\n",
      "Epoch: 2, Loss: 0.6573159481001584\n",
      "Epoch: 3, Loss: 0.6469882182448523\n",
      "Epoch: 4, Loss: 0.6370493802715192\n",
      "Epoch: 5, Loss: 0.62748117227206\n",
      "Epoch: 6, Loss: 0.6182669971424355\n",
      "Epoch: 7, Loss: 0.6093909441437486\n",
      "Epoch: 8, Loss: 0.6008377131815117\n",
      "Epoch: 9, Loss: 0.5925926121098976\n",
      "Epoch: 10, Loss: 0.5846415556797371\n",
      "Epoch: 11, Loss: 0.576971060721128\n",
      "Epoch: 12, Loss: 0.5695682377086851\n",
      "Epoch: 13, Loss: 0.5624207792724467\n",
      "Epoch: 14, Loss: 0.555516946210792\n",
      "Epoch: 15, Loss: 0.5488455515161964\n",
      "Epoch: 16, Loss: 0.5423959428718335\n",
      "Epoch: 17, Loss: 0.5361579840226333\n",
      "Epoch: 18, Loss: 0.530122035371411\n",
      "Epoch: 19, Loss: 0.524278934100767\n",
      "Epoch: 20, Loss: 0.5186199740755187\n",
      "Epoch: 21, Loss: 0.513136885738823\n",
      "Epoch: 22, Loss: 0.5078218161779645\n",
      "Epoch: 23, Loss: 0.5026673095029317\n",
      "Epoch: 24, Loss: 0.4976662876521384\n",
      "Epoch: 25, Loss: 0.49281203171472465\n",
      "Epoch: 26, Loss: 0.4880981638374402\n",
      "Epoch: 27, Loss: 0.48351862976586396\n",
      "Epoch: 28, Loss: 0.4790676820542955\n",
      "Epoch: 29, Loss: 0.47473986396575446\n",
      "Epoch: 30, Loss: 0.4705299940728303\n",
      "Epoch: 31, Loss: 0.4664331515613508\n",
      "Epoch: 32, Loss: 0.46244466223173436\n",
      "Epoch: 33, Loss: 0.4585600851872111\n",
      "Epoch: 34, Loss: 0.4547752001936282\n",
      "Epoch: 35, Loss: 0.45108599569213714\n",
      "Epoch: 36, Loss: 0.4474886574434849\n",
      "Epoch: 37, Loss: 0.44397955778080567\n",
      "Epoch: 38, Loss: 0.44055524544656577\n",
      "Epoch: 39, Loss: 0.43721243598858334\n",
      "Epoch: 40, Loss: 0.4339480026897017\n",
      "Epoch: 41, Loss: 0.4307589680056913\n",
      "Epoch: 42, Loss: 0.4276424954862072\n",
      "Epoch: 43, Loss: 0.4245958821540798\n",
      "Epoch: 44, Loss: 0.4216165513188392\n",
      "Epoch: 45, Loss: 0.4187020458011006\n",
      "Epoch: 46, Loss: 0.41585002154526296\n",
      "Epoch: 47, Loss: 0.41305824159885257\n",
      "Epoch: 48, Loss: 0.4103245704377623\n",
      "Epoch: 49, Loss: 0.40764696861757954\n",
      "Epoch: 50, Loss: 0.4050234877321394\n",
      "Epoch: 51, Loss: 0.4024522656613835\n",
      "Epoch: 52, Loss: 0.3999315220915329\n",
      "Epoch: 53, Loss: 0.39745955429148794\n",
      "Epoch: 54, Loss: 0.3950347331302518\n",
      "Epoch: 55, Loss: 0.39265549932102284\n",
      "Epoch: 56, Loss: 0.39032035987842145\n",
      "Epoch: 57, Loss: 0.3880278847760987\n",
      "Epoch: 58, Loss: 0.3857767037927245\n",
      "Epoch: 59, Loss: 0.38356550353506363\n",
      "Epoch: 60, Loss: 0.3813930246275275\n",
      "Epoch: 61, Loss: 0.3792580590582266\n",
      "Epoch: 62, Loss: 0.3771594476721598\n",
      "Epoch: 63, Loss: 0.375096077802746\n",
      "Epoch: 64, Loss: 0.37306688103344754\n",
      "Epoch: 65, Loss: 0.3710708310817393\n",
      "Epoch: 66, Loss: 0.36910694179816117\n",
      "Epoch: 67, Loss: 0.3671742652736389\n",
      "Epoch: 68, Loss: 0.36527189004868166\n",
      "Epoch: 69, Loss: 0.36339893941846435\n",
      "Epoch: 70, Loss: 0.361554569828171\n",
      "Epoch: 71, Loss: 0.35973796935332886\n",
      "Epoch: 72, Loss: 0.35794835626018867\n",
      "Epoch: 73, Loss: 0.35618497764151386\n",
      "Epoch: 74, Loss: 0.3544471081234277\n",
      "Epoch: 75, Loss: 0.35273404863923974\n",
      "Epoch: 76, Loss: 0.35104512526642084\n",
      "Epoch: 77, Loss: 0.34937968812313674\n",
      "Epoch: 78, Loss: 0.3477371103209665\n",
      "Epoch: 79, Loss: 0.34611678697064285\n",
      "Epoch: 80, Loss: 0.3445181342378436\n",
      "Epoch: 81, Loss: 0.342940588446243\n",
      "Epoch: 82, Loss: 0.3413836052252055\n",
      "Epoch: 83, Loss: 0.3398466586996596\n",
      "Epoch: 84, Loss: 0.3383292407198394\n",
      "Epoch: 85, Loss: 0.33683086012872265\n",
      "Epoch: 86, Loss: 0.33535104206512156\n",
      "Epoch: 87, Loss: 0.3338893273005069\n",
      "Epoch: 88, Loss: 0.33244527160776033\n",
      "Epoch: 89, Loss: 0.33101844516015577\n",
      "Epoch: 90, Loss: 0.3296084319589738\n",
      "Epoch: 91, Loss: 0.32821482928824436\n",
      "Epoch: 92, Loss: 0.3268372471952039\n",
      "Epoch: 93, Loss: 0.32547530799513524\n",
      "Epoch: 94, Loss: 0.32412864579933565\n",
      "Epoch: 95, Loss: 0.322796906065033\n",
      "Epoch: 96, Loss: 0.32147974516613703\n",
      "Epoch: 97, Loss: 0.3201768299837788\n",
      "Epoch: 98, Loss: 0.3188878375156506\n",
      "Epoch: 99, Loss: 0.31761245450321607\n",
      "Fold 4 with alpha: 10\n",
      "(609, 7) (609,) (153, 7) (153,)\n",
      "Epoch: 0, Loss: 0.6821277587030274\n",
      "Epoch: 1, Loss: 0.6722765168089012\n",
      "Epoch: 2, Loss: 0.6627996737595782\n",
      "Epoch: 3, Loss: 0.6536232093954708\n",
      "Epoch: 4, Loss: 0.6447263303424853\n",
      "Epoch: 5, Loss: 0.6360929816992633\n",
      "Epoch: 6, Loss: 0.6277082503635782\n",
      "Epoch: 7, Loss: 0.6195580450854319\n",
      "Epoch: 8, Loss: 0.6116290218507132\n",
      "Epoch: 9, Loss: 0.6039085333408897\n",
      "Epoch: 10, Loss: 0.5963845847254046\n",
      "Epoch: 11, Loss: 0.5890457931545365\n",
      "Epoch: 12, Loss: 0.5818813499751154\n",
      "Epoch: 13, Loss: 0.5748809851755082\n",
      "Epoch: 14, Loss: 0.568034933785692\n",
      "Epoch: 15, Loss: 0.5613339040600073\n",
      "Epoch: 16, Loss: 0.5547690473185448\n",
      "Epoch: 17, Loss: 0.5483319293487294\n",
      "Epoch: 18, Loss: 0.5420145032849103\n",
      "Epoch: 19, Loss: 0.535809083896248\n",
      "Epoch: 20, Loss: 0.5297083232243276\n",
      "Epoch: 21, Loss: 0.5237051875229415\n",
      "Epoch: 22, Loss: 0.5177929354642171\n",
      "Epoch: 23, Loss: 0.511965097588402\n",
      "Epoch: 24, Loss: 0.5062154569899604\n",
      "Epoch: 25, Loss: 0.5005380312510348\n",
      "Epoch: 26, Loss: 0.49492705565590067\n",
      "Epoch: 27, Loss: 0.4893769677481424\n",
      "Epoch: 28, Loss: 0.48388239332766675\n",
      "Epoch: 29, Loss: 0.4784381340296414\n",
      "Epoch: 30, Loss: 0.4730391566849873\n",
      "Epoch: 31, Loss: 0.46768058473615953\n",
      "Epoch: 32, Loss: 0.46235769207790284\n",
      "Epoch: 33, Loss: 0.4570658998175704\n",
      "Epoch: 34, Loss: 0.451800776613043\n",
      "Epoch: 35, Loss: 0.44655804346125283\n",
      "Epoch: 36, Loss: 0.4413335840945731\n",
      "Epoch: 37, Loss: 0.43612346252026035\n",
      "Epoch: 38, Loss: 0.43092394974345116\n",
      "Epoch: 39, Loss: 0.4257315623939253\n",
      "Epoch: 40, Loss: 0.4205431168968564\n",
      "Epoch: 41, Loss: 0.4153558040813238\n",
      "Epoch: 42, Loss: 0.4101672908403679\n",
      "Epoch: 43, Loss: 0.4049758578339754\n",
      "Epoch: 44, Loss: 0.3997805855389655\n",
      "Epoch: 45, Loss: 0.3945816056037602\n",
      "Epoch: 46, Loss: 0.3893804410630325\n",
      "Epoch: 47, Loss: 0.3841804684073587\n",
      "Epoch: 48, Loss: 0.3789875481483241\n",
      "Epoch: 49, Loss: 0.3738108904562566\n",
      "Epoch: 50, Loss: 0.3686642519121911\n",
      "Epoch: 51, Loss: 0.36356760349670114\n",
      "Epoch: 52, Loss: 0.3585494767540225\n",
      "Epoch: 53, Loss: 0.3536502977709974\n",
      "Epoch: 54, Loss: 0.3489271788509056\n",
      "Epoch: 55, Loss: 0.3444608918284975\n",
      "Epoch: 56, Loss: 0.34036615682628\n",
      "Epoch: 57, Loss: 0.33680705387521\n",
      "Epoch: 58, Loss: 0.3340204946238343\n",
      "Epoch: 59, Loss: 0.3323526286837942\n",
      "Epoch: 60, Loss: 0.3323164633988104\n",
      "Epoch: 61, Loss: 0.3346851237061399\n",
      "Stopping early due to increase in loss.\n",
      "Fold 5 with alpha: 100\n",
      "(609, 7) (609,) (153, 7) (153,)\n",
      "Epoch: 0, Loss: 0.678985025091137\n",
      "Epoch: 1, Loss: 0.668417659081934\n",
      "Epoch: 2, Loss: 0.6584512941627448\n",
      "Epoch: 3, Loss: 0.6487494953698482\n",
      "Epoch: 4, Loss: 0.6392109554289094\n",
      "Epoch: 5, Loss: 0.6297569256663134\n",
      "Epoch: 6, Loss: 0.6203101074755679\n",
      "Epoch: 7, Loss: 0.6107905557430341\n",
      "Epoch: 8, Loss: 0.6011132139458498\n",
      "Epoch: 9, Loss: 0.5911857165071265\n",
      "Epoch: 10, Loss: 0.580906561679151\n",
      "Epoch: 11, Loss: 0.5701644597375566\n",
      "Epoch: 12, Loss: 0.558841150046288\n",
      "Epoch: 13, Loss: 0.5468238172276558\n",
      "Epoch: 14, Loss: 0.5340437058292585\n",
      "Epoch: 15, Loss: 0.5205879253005252\n",
      "Epoch: 16, Loss: 0.5070266136219993\n",
      "Epoch: 17, Loss: 0.4954244852700511\n",
      "Epoch: 18, Loss: 0.4927584539984975\n",
      "Epoch: 19, Loss: 0.5239076564025434\n",
      "Stopping early due to increase in loss.\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data\n",
    "\n",
    "shuffled_data = data_rice.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def kfold_split(data, k):\n",
    "    data_split = []\n",
    "    fold_size = int(len(data) / k)\n",
    "    for i in range(k):\n",
    "        fold = data[i * fold_size: (i + 1) * fold_size]\n",
    "        data_split.append(fold)\n",
    "    return data_split\n",
    "\n",
    "k = 5\n",
    "data_folds = kfold_split(shuffled_data, k)\n",
    "\n",
    "alpha_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "all_loss = []\n",
    "all_accuracy = []\n",
    "\n",
    "lr = 1\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    # Define the split size for the training set\n",
    "    train_size = int(0.8 * len(data_folds[i]))  # 80% of data for training, 20% for testing\n",
    "    # Split the data\n",
    "    train_data = data_folds[i][:train_size]\n",
    "    test_data = data_folds[i][train_size:]\n",
    "    \n",
    "    X_train = train_data.drop('Class', axis=1).to_numpy()  # Convert DataFrame to numpy array\n",
    "    y_train = train_data['Class'].to_numpy()  # Convert Series to numpy array\n",
    "\n",
    "    X_test = test_data.drop('Class', axis=1).to_numpy()  # Convert DataFrame to numpy array\n",
    "    y_test = test_data['Class'].to_numpy()  # Convert Series to numpy array\n",
    "\n",
    "    w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "    alpha = alpha_values[i]\n",
    "\n",
    "    print(f'Fold {i+1} with alpha: {alpha}')\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    w, b, losses, training_time = train_batch_gd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "    test_loss = test(X_test, y_test, w, b)\n",
    "\n",
    "    all_loss.append(test_loss)\n",
    "\n",
    "    # test accuracy\n",
    "    y_pred = predict(X_test, w, b)\n",
    "    test_accuracy = np.mean(y_pred == y_test)\n",
    "    all_accuracy.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3238317221555476,\n",
       " 0.30477893195127603,\n",
       " 0.28080375827563403,\n",
       " 0.26996408761963453,\n",
       " 0.5461734699480586]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9084967320261438,\n",
       " 0.9215686274509803,\n",
       " 0.954248366013072,\n",
       " 0.9150326797385621,\n",
       " 0.6535947712418301]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6814436551651445\n",
      "Epoch: 1, Loss: 0.6703319495003883\n",
      "Epoch: 2, Loss: 0.6596370000517682\n",
      "Epoch: 3, Loss: 0.649330204390798\n",
      "Epoch: 4, Loss: 0.6393949037894456\n",
      "Epoch: 5, Loss: 0.6298157794862771\n",
      "Epoch: 6, Loss: 0.6205780398207238\n",
      "Epoch: 7, Loss: 0.6116673740598673\n",
      "Epoch: 8, Loss: 0.603069961071874\n",
      "Epoch: 9, Loss: 0.5947724774872758\n",
      "Epoch: 10, Loss: 0.5867621016239115\n",
      "Epoch: 11, Loss: 0.5790265134450276\n",
      "Epoch: 12, Loss: 0.5715538910919974\n",
      "Epoch: 13, Loss: 0.5643329045170489\n",
      "Epoch: 14, Loss: 0.5573527067007311\n",
      "Epoch: 15, Loss: 0.5506029228943837\n",
      "Epoch: 16, Loss: 0.5440736382826377\n",
      "Epoch: 17, Loss: 0.537755384416481\n",
      "Epoch: 18, Loss: 0.531639124724716\n",
      "Epoch: 19, Loss: 0.5257162393714286\n",
      "Epoch: 20, Loss: 0.5199785096898399\n",
      "Epoch: 21, Loss: 0.5144181023888253\n",
      "Epoch: 22, Loss: 0.5090275536975944\n",
      "Epoch: 23, Loss: 0.5037997535864557\n",
      "Epoch: 24, Loss: 0.49872793017715966\n",
      "Epoch: 25, Loss: 0.4938056344348368\n",
      "Epoch: 26, Loss: 0.48902672521483087\n",
      "Epoch: 27, Loss: 0.4843853547215204\n",
      "Epoch: 28, Loss: 0.4798759544223269\n",
      "Epoch: 29, Loss: 0.475493221448256\n",
      "Epoch: 30, Loss: 0.4712321055022995\n",
      "Epoch: 31, Loss: 0.46708779628863434\n",
      "Epoch: 32, Loss: 0.46305571146856706\n",
      "Epoch: 33, Loss: 0.4591314851434337\n",
      "Epoch: 34, Loss: 0.4553109568599853\n",
      "Epoch: 35, Loss: 0.4515901611300358\n",
      "Epoch: 36, Loss: 0.4479653174531781\n",
      "Epoch: 37, Loss: 0.4444328208290778\n",
      "Epoch: 38, Loss: 0.44098923274411717\n",
      "Epoch: 39, Loss: 0.43763127261590395\n",
      "Epoch: 40, Loss: 0.43435580967829984\n",
      "Epoch: 41, Loss: 0.43115985528908857\n",
      "Epoch: 42, Loss: 0.4280405556421426\n",
      "Epoch: 43, Loss: 0.42499518486590465\n",
      "Epoch: 44, Loss: 0.42202113849013806\n",
      "Epoch: 45, Loss: 0.4191159272631756\n",
      "Epoch: 46, Loss: 0.41627717130228775\n",
      "Epoch: 47, Loss: 0.4135025945602624\n",
      "Epoch: 48, Loss: 0.4107900195918291\n",
      "Epoch: 49, Loss: 0.4081373626041437\n",
      "Epoch: 50, Loss: 0.4055426287761608\n",
      "Epoch: 51, Loss: 0.40300390783236334\n",
      "Epoch: 52, Loss: 0.40051936985695086\n",
      "Epoch: 53, Loss: 0.39808726133524086\n",
      "Epoch: 54, Loss: 0.3957059014096707\n",
      "Epoch: 55, Loss: 0.393373678338415\n",
      "Epoch: 56, Loss: 0.39108904614524975\n",
      "Epoch: 57, Loss: 0.3888505214498853\n",
      "Epoch: 58, Loss: 0.38665668046857005\n",
      "Epoch: 59, Loss: 0.38450615617531764\n",
      "Epoch: 60, Loss: 0.3823976356146429\n",
      "Epoch: 61, Loss: 0.3803298573572032\n",
      "Epoch: 62, Loss: 0.37830160909022353\n",
      "Epoch: 63, Loss: 0.3763117253350468\n",
      "Epoch: 64, Loss: 0.37435908528459416\n",
      "Epoch: 65, Loss: 0.3724426107539294\n",
      "Epoch: 66, Loss: 0.3705612642375242\n",
      "Epoch: 67, Loss: 0.3687140470671888\n",
      "Epoch: 68, Loss: 0.3668999976649883\n",
      "Epoch: 69, Loss: 0.365118189885798\n",
      "Epoch: 70, Loss: 0.36336773144446344\n",
      "Epoch: 71, Loss: 0.361647762422832\n",
      "Epoch: 72, Loss: 0.3599574538521938\n",
      "Epoch: 73, Loss: 0.35829600636694287\n",
      "Epoch: 74, Loss: 0.3566626489255064\n",
      "Epoch: 75, Loss: 0.3550566375948316\n",
      "Epoch: 76, Loss: 0.3534772543949301\n",
      "Epoch: 77, Loss: 0.3519238062001929\n",
      "Epoch: 78, Loss: 0.35039562369437594\n",
      "Epoch: 79, Loss: 0.34889206037634274\n",
      "Epoch: 80, Loss: 0.3474124916138161\n",
      "Epoch: 81, Loss: 0.3459563137425566\n",
      "Epoch: 82, Loss: 0.34452294320853066\n",
      "Epoch: 83, Loss: 0.3431118157507762\n",
      "Epoch: 84, Loss: 0.341722385622805\n",
      "Epoch: 85, Loss: 0.34035412485050426\n",
      "Epoch: 86, Loss: 0.3390065225246207\n",
      "Epoch: 87, Loss: 0.3376790841260155\n",
      "Epoch: 88, Loss: 0.33637133088198556\n",
      "Epoch: 89, Loss: 0.3350827991520424\n",
      "Epoch: 90, Loss: 0.3338130398416298\n",
      "Epoch: 91, Loss: 0.3325616178423496\n",
      "Epoch: 92, Loss: 0.3313281114973425\n",
      "Epoch: 93, Loss: 0.33011211209054914\n",
      "Epoch: 94, Loss: 0.32891322335864637\n",
      "Epoch: 95, Loss: 0.32773106102451965\n",
      "Epoch: 96, Loss: 0.3265652523511987\n",
      "Epoch: 97, Loss: 0.3254154357152379\n",
      "Epoch: 98, Loss: 0.3242812601985832\n",
      "Epoch: 99, Loss: 0.3231623851980168\n",
      "Epoch: 100, Loss: 0.3220584800513202\n",
      "Epoch: 101, Loss: 0.3209692236793451\n",
      "Epoch: 102, Loss: 0.3198943042432213\n",
      "Epoch: 103, Loss: 0.3188334188159766\n",
      "Epoch: 104, Loss: 0.31778627306787727\n",
      "Epoch: 105, Loss: 0.31675258096483944\n",
      "Epoch: 106, Loss: 0.3157320644792916\n",
      "Epoch: 107, Loss: 0.31472445331290355\n",
      "Epoch: 108, Loss: 0.31372948463062766\n",
      "Epoch: 109, Loss: 0.31274690280552475\n",
      "Epoch: 110, Loss: 0.3117764591738777\n",
      "Epoch: 111, Loss: 0.3108179118001181\n",
      "Epoch: 112, Loss: 0.30987102525111826\n",
      "Epoch: 113, Loss: 0.30893557037942215\n",
      "Epoch: 114, Loss: 0.30801132411500953\n",
      "Epoch: 115, Loss: 0.3070980692652126\n",
      "Epoch: 116, Loss: 0.3061955943224162\n",
      "Epoch: 117, Loss: 0.3053036932791991\n",
      "Epoch: 118, Loss: 0.30442216545058365\n",
      "Epoch: 119, Loss: 0.30355081530308325\n",
      "Epoch: 120, Loss: 0.3026894522902491\n",
      "Epoch: 121, Loss: 0.30183789069443273\n",
      "Epoch: 122, Loss: 0.30099594947449626\n",
      "Epoch: 123, Loss: 0.30016345211921286\n",
      "Epoch: 124, Loss: 0.2993402265061149\n",
      "Epoch: 125, Loss: 0.2985261047655564\n",
      "Epoch: 126, Loss: 0.29772092314976967\n",
      "Epoch: 127, Loss: 0.29692452190670515\n",
      "Epoch: 128, Loss: 0.2961367451584542\n",
      "Epoch: 129, Loss: 0.29535744078406256\n",
      "Epoch: 130, Loss: 0.2945864603065548\n",
      "Epoch: 131, Loss: 0.29382365878399197\n",
      "Epoch: 132, Loss: 0.2930688947044017\n",
      "Epoch: 133, Loss: 0.29232202988441797\n",
      "Epoch: 134, Loss: 0.29158292937148383\n",
      "Epoch: 135, Loss: 0.29085146134946926\n",
      "Epoch: 136, Loss: 0.2901274970475708\n",
      "Epoch: 137, Loss: 0.2894109106523582\n",
      "Epoch: 138, Loss: 0.28870157922284523\n",
      "Epoch: 139, Loss: 0.28799938260846414\n",
      "Epoch: 140, Loss: 0.2873042033698289\n",
      "Epoch: 141, Loss: 0.286615926702179\n",
      "Epoch: 142, Loss: 0.28593444036139876\n",
      "Epoch: 143, Loss: 0.28525963459251213\n",
      "Epoch: 144, Loss: 0.2845914020605581\n",
      "Epoch: 145, Loss: 0.2839296377837548\n",
      "Epoch: 146, Loss: 0.28327423906886473\n",
      "Epoch: 147, Loss: 0.2826251054486786\n",
      "Epoch: 148, Loss: 0.2819821386215361\n",
      "Epoch: 149, Loss: 0.28134524239280845\n",
      "Epoch: 150, Loss: 0.2807143226182681\n",
      "Epoch: 151, Loss: 0.2800892871492765\n",
      "Epoch: 152, Loss: 0.279470045779721\n",
      "Epoch: 153, Loss: 0.27885651019463836\n",
      "Epoch: 154, Loss: 0.27824859392046125\n",
      "Epoch: 155, Loss: 0.2776462122768294\n",
      "Epoch: 156, Loss: 0.277049282329908\n",
      "Epoch: 157, Loss: 0.27645772284715986\n",
      "Epoch: 158, Loss: 0.27587145425351783\n",
      "Epoch: 159, Loss: 0.2752903985889074\n",
      "Epoch: 160, Loss: 0.27471447946707345\n",
      "Epoch: 161, Loss: 0.27414362203566095\n",
      "Epoch: 162, Loss: 0.2735777529375099\n",
      "Epoch: 163, Loss: 0.27301680027311814\n",
      "Epoch: 164, Loss: 0.27246069356423314\n",
      "Epoch: 165, Loss: 0.2719093637185336\n",
      "Epoch: 166, Loss: 0.27136274299536156\n",
      "Epoch: 167, Loss: 0.2708207649724709\n",
      "Epoch: 168, Loss: 0.27028336451375534\n",
      "Epoch: 169, Loss: 0.2697504777379241\n",
      "Epoch: 170, Loss: 0.26922204198809246\n",
      "Epoch: 171, Loss: 0.26869799580225523\n",
      "Epoch: 172, Loss: 0.26817827888461576\n",
      "Epoch: 173, Loss: 0.2676628320777394\n",
      "Epoch: 174, Loss: 0.2671515973355052\n",
      "Epoch: 175, Loss: 0.26664451769682923\n",
      "Epoch: 176, Loss: 0.26614153726013373\n",
      "Epoch: 177, Loss: 0.2656426011585379\n",
      "Epoch: 178, Loss: 0.2651476555357458\n",
      "Epoch: 179, Loss: 0.2646566475226104\n",
      "Epoch: 180, Loss: 0.26416952521434967\n",
      "Epoch: 181, Loss: 0.2636862376483953\n",
      "Epoch: 182, Loss: 0.26320673478285306\n",
      "Epoch: 183, Loss: 0.262730967475555\n",
      "Epoch: 184, Loss: 0.2622588874636852\n",
      "Epoch: 185, Loss: 0.26179044734396006\n",
      "Epoch: 186, Loss: 0.26132560055334697\n",
      "Epoch: 187, Loss: 0.26086430135030225\n",
      "Epoch: 188, Loss: 0.2604065047965146\n",
      "Epoch: 189, Loss: 0.25995216673913635\n",
      "Epoch: 190, Loss: 0.2595012437934883\n",
      "Epoch: 191, Loss: 0.2590536933262233\n",
      "Epoch: 192, Loss: 0.25860947343893426\n",
      "Epoch: 193, Loss: 0.25816854295219366\n",
      "Epoch: 194, Loss: 0.25773086139000995\n",
      "Epoch: 195, Loss: 0.25729638896469004\n",
      "Epoch: 196, Loss: 0.25686508656209417\n",
      "Epoch: 197, Loss: 0.2564369157272719\n",
      "Epoch: 198, Loss: 0.2560118386504675\n",
      "Epoch: 199, Loss: 0.25558981815348425\n",
      "Test Loss: 0.2725582917364914\n",
      "Accuracy: 0.9120734908136483\n"
     ]
    }
   ],
   "source": [
    "# we see that accuracy is maximum for alpha = 1 hence we will use this value for our model\n",
    "# let's check the performance of our model on the entire dataset\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(data_rice)\n",
    "\n",
    "w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "alpha = 1\n",
    "lr = 1\n",
    "epochs = 200\n",
    "\n",
    "w, b, losses, training_time = train_batch_gd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "test_loss = test(X_test, y_test, w, b)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "loss_dict[\"L2-GD\"] = test_loss\n",
    "\n",
    "\n",
    "\n",
    "# test accuracy\n",
    "y_pred = predict(X_test.to_numpy(), w, b)\n",
    "test_accuracy = np.mean(y_pred == y_test.to_numpy())\n",
    "print(f'Accuracy: {test_accuracy}')\n",
    "\n",
    "# training accuracy\n",
    "y_pred = predict(X_train.to_numpy(), w, b)\n",
    "training_accuracy = np.mean(y_pred == y_train.to_numpy())\n",
    "\n",
    "accuracy_dict[\"L2-GD\"] = (training_accuracy, test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now the thing is for lambda (regularization parameter) 0 learning rate 1 works the best as accuracy, but for lambda 10 learning rate 0.1 works better. Since the accuracy difference is not that big, i will use learning rate 1 for batch gradient descent linear regression not for only consistency but also test loss is less than the lr 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GD': (0.9248687664041995, 0.9186351706036745),\n",
       " 'L2-GD': (0.9219160104986877, 0.9120734908136483),\n",
       " 'SGD': (0.9324146981627297, 0.9173228346456693),\n",
       " 'L2-SGD': (0.9301181102362205, 0.9094488188976378)}"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now it's time for Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i will be using learning rate 0.1 for SGD, since it's faster than BGD and i the step size can get so big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.3022313820483077\n",
      "Epoch: 1, Loss: 0.20192423762193226\n",
      "Epoch: 2, Loss: 0.18479356152601026\n",
      "Epoch: 3, Loss: 0.17736988634505738\n",
      "Epoch: 4, Loss: 0.1743647614552086\n",
      "Epoch: 5, Loss: 0.17245424728426853\n",
      "Epoch: 6, Loss: 0.17051567477265261\n",
      "Epoch: 7, Loss: 0.16967360635224477\n",
      "Epoch: 8, Loss: 0.16870075700502946\n",
      "Epoch: 9, Loss: 0.16927571529449187\n",
      "Stopping early due to increase in average loss.\n",
      "Test loss: 0.20325701490096004\n",
      "Accuracy: 0.916010498687664\n"
     ]
    }
   ],
   "source": [
    "# initialize the weights\n",
    "w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "alpha = 0\n",
    "lr = 0.1\n",
    "epochs = 200\n",
    "w, b, losses, training_time = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "test_loss = test(X_test, y_test, w, b)\n",
    "print(f'Test loss: {test_loss}')\n",
    "loss_dict[\"SGD\"] = test_loss\n",
    "\n",
    "# test accuracy\n",
    "y_pred = predict(X_test.to_numpy(), w, b)\n",
    "test_accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Accuracy: {test_accuracy}')\n",
    "\n",
    "# training accuracy\n",
    "y_pred = predict(X_train.to_numpy(), w, b)\n",
    "training_accuracy = np.mean(y_pred == y_train)\n",
    "\n",
    "accuracy_dict[\"SGD\"] = (training_accuracy, test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 6.333058736910881\n",
      "Epoch: 0, Loss: 0.4607307075052127\n",
      "Epoch: 1, Loss: 0.3182240134345797\n",
      "Epoch: 2, Loss: 0.2624521396948246\n",
      "Epoch: 3, Loss: 0.24038393173688805\n",
      "Epoch: 4, Loss: 0.22280600127502187\n",
      "Epoch: 5, Loss: 0.21051950239829853\n",
      "Epoch: 6, Loss: 0.2013553508356876\n",
      "Epoch: 7, Loss: 0.19716078640847465\n",
      "Epoch: 8, Loss: 0.19002688758596753\n",
      "Epoch: 9, Loss: 0.18741320810687562\n",
      "Epoch: 10, Loss: 0.18282257466549848\n",
      "Epoch: 11, Loss: 0.1804403451539406\n",
      "Epoch: 12, Loss: 0.17486853064539898\n",
      "Epoch: 13, Loss: 0.17684898159929718\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.522104967861952\n",
      "Epoch: 0, Loss: 0.4597863759596947\n",
      "Epoch: 1, Loss: 0.31321629064937134\n",
      "Epoch: 2, Loss: 0.2611206910230138\n",
      "Epoch: 3, Loss: 0.23505649659381922\n",
      "Epoch: 4, Loss: 0.21820977789690363\n",
      "Epoch: 5, Loss: 0.20738681427423028\n",
      "Epoch: 6, Loss: 0.1959025818366836\n",
      "Epoch: 7, Loss: 0.19000153854931515\n",
      "Epoch: 8, Loss: 0.1855162877130681\n",
      "Epoch: 9, Loss: 0.17916174722132536\n",
      "Epoch: 10, Loss: 0.1761622905090327\n",
      "Epoch: 11, Loss: 0.17166765839362258\n",
      "Epoch: 12, Loss: 0.16964409886566947\n",
      "Epoch: 13, Loss: 0.16884560322548442\n",
      "Epoch: 14, Loss: 0.1669491340333579\n",
      "Epoch: 15, Loss: 0.16837031357321966\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.8056743142885585\n",
      "Epoch: 0, Loss: 0.4565625821410171\n",
      "Epoch: 1, Loss: 0.30917659398259395\n",
      "Epoch: 2, Loss: 0.24712381812502054\n",
      "Epoch: 3, Loss: 0.21565777570095265\n",
      "Epoch: 4, Loss: 3.408134434211653\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.654437329527701\n",
      "Epoch: 0, Loss: 0.4481462693445926\n",
      "Epoch: 1, Loss: nan\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.881292806668986\n",
      "Epoch: 0, Loss: 5.032049249903788\n",
      "Epoch: 1, Loss: 6.881292806668986\n",
      "Stopping early due to increase in average loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/b29xgfjn237gd2tgvs743q8m0000gn/T/ipykernel_47040/2022709499.py:24: RuntimeWarning: overflow encountered in multiply\n",
      "  dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w*w)/N)\n",
      "/var/folders/sj/b29xgfjn237gd2tgvs743q8m0000gn/T/ipykernel_47040/2022709499.py:24: RuntimeWarning: overflow encountered in multiply\n",
      "  dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w*w)/N)\n",
      "/var/folders/sj/b29xgfjn237gd2tgvs743q8m0000gn/T/ipykernel_47040/2022709499.py:24: RuntimeWarning: overflow encountered in multiply\n",
      "  dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w*w)/N)\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "data_folds = kfold_split(shuffled_data, k)\n",
    "\n",
    "alpha_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "all_loss = []\n",
    "\n",
    "all_accuracy = []\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 50\n",
    "\n",
    "for i in range(k):\n",
    "    # Define the split size for the training set\n",
    "    train_size = int(0.8 * len(data_folds[i]))  # 80% of data for training, 20% for testing\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data_folds[i][:train_size]\n",
    "    test_data = data_folds[i][train_size:]\n",
    "\n",
    "    X_train = train_data.drop('Class', axis=1).to_numpy()  # Replace 'target_column' with your actual target column name\n",
    "    y_train = train_data['Class'].to_numpy()  # Convert Series to numpy array\n",
    "\n",
    "    X_test = test_data.drop('Class', axis=1).to_numpy()\n",
    "    y_test = test_data['Class'].to_numpy()\n",
    "\n",
    "    w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "    alpha = alpha_values[i]\n",
    "\n",
    "\n",
    "    w, b, losses, training_time = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "    test_loss = test(X_test, y_test, w, b)\n",
    "\n",
    "    all_loss.append(test_loss)\n",
    "\n",
    "    y_pred = predict(X_test, w, b)\n",
    "    acc = np.mean(y_pred == y_test)\n",
    "    all_accuracy.append(acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1933496632545962, 0.16501357105549028, nan, nan, 10.594901347175217]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8954248366013072,\n",
       " 0.9281045751633987,\n",
       " 0.45751633986928103,\n",
       " 0.45098039215686275,\n",
       " 0.42483660130718953]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the best accuracy comes from lambda 0.1, the loss diverges for lambda >= 1. also i tested the learning rates here and the 0.1 is the best for not diverging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data refreshing\n",
    "shuffled_data = data_rice.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Define the split size for the training set\n",
    "train_size = int(0.8 * len(shuffled_data))  # 80% of data for training, 20% for testing\n",
    "\n",
    "# Split the data\n",
    "train_data = shuffled_data[:train_size]\n",
    "test_data = shuffled_data[train_size:]\n",
    "\n",
    "X_train = train_data.drop('Class', axis=1)  # Replace 'target_column' with your actual target column name\n",
    "y_train = train_data['Class']\n",
    "\n",
    "X_test = test_data.drop('Class', axis=1)\n",
    "y_test = test_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.30304179615496313\n",
      "Epoch: 1, Loss: 0.2007522058588852\n",
      "Epoch: 2, Loss: 0.1845914524234946\n",
      "Epoch: 3, Loss: 0.17505808097996997\n",
      "Epoch: 4, Loss: 0.1719092319379568\n",
      "Epoch: 5, Loss: 0.16819741517479694\n",
      "Epoch: 6, Loss: 0.16892513839415518\n",
      "Stopping early due to increase in average loss.\n",
      "Test loss: 0.2054584223600729\n",
      "Accuracy: 0.9146981627296588\n"
     ]
    }
   ],
   "source": [
    "# initialize the weights\n",
    "w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "alpha = 0.1\n",
    "lr = 0.1\n",
    "epochs = 200\n",
    "w, b, losses, training_time = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "test_loss = test(X_test, y_test, w, b)\n",
    "print(f'Test loss: {test_loss}')\n",
    "loss_dict[\"L2-SGD\"] = test_loss\n",
    "\n",
    "# test accuracy\n",
    "y_pred = predict(X_test.to_numpy(), w, b)\n",
    "test_accuracy = np.mean(y_pred == y_test.to_numpy())\n",
    "print(f'Accuracy: {test_accuracy}')\n",
    "\n",
    "# training accuracy\n",
    "y_pred = predict(X_train.to_numpy(), w, b)\n",
    "training_accuracy = np.mean(y_pred == y_train.to_numpy())\n",
    "\n",
    "accuracy_dict[\"L2-SGD\"] = (training_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GD': (0.9248687664041995, 0.9186351706036745),\n",
       " 'L2-GD': (0.9219160104986877, 0.9120734908136483),\n",
       " 'SGD': (0.9333989501312336, 0.916010498687664),\n",
       " 'L2-SGD': (0.9333989501312336, 0.9146981627296588)}"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's compare the times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6814436551651445\n",
      "Epoch: 1, Loss: 0.6703319495003883\n",
      "Epoch: 2, Loss: 0.6596370000517682\n",
      "Epoch: 3, Loss: 0.649330204390798\n",
      "Epoch: 4, Loss: 0.6393949037894456\n",
      "Epoch: 5, Loss: 0.6298157794862771\n",
      "Epoch: 6, Loss: 0.6205780398207238\n",
      "Epoch: 7, Loss: 0.6116673740598673\n",
      "Epoch: 8, Loss: 0.603069961071874\n",
      "Epoch: 9, Loss: 0.5947724774872758\n",
      "Epoch: 10, Loss: 0.5867621016239115\n",
      "Epoch: 11, Loss: 0.5790265134450276\n",
      "Epoch: 12, Loss: 0.5715538910919974\n",
      "Epoch: 13, Loss: 0.5643329045170489\n",
      "Epoch: 14, Loss: 0.5573527067007311\n",
      "Epoch: 15, Loss: 0.5506029228943837\n",
      "Epoch: 16, Loss: 0.5440736382826377\n",
      "Epoch: 17, Loss: 0.537755384416481\n",
      "Epoch: 18, Loss: 0.531639124724716\n",
      "Epoch: 19, Loss: 0.5257162393714286\n",
      "Epoch: 20, Loss: 0.5199785096898399\n",
      "Epoch: 21, Loss: 0.5144181023888253\n",
      "Epoch: 22, Loss: 0.5090275536975944\n",
      "Epoch: 23, Loss: 0.5037997535864557\n",
      "Epoch: 24, Loss: 0.49872793017715966\n",
      "Epoch: 25, Loss: 0.4938056344348368\n",
      "Epoch: 26, Loss: 0.48902672521483087\n",
      "Epoch: 27, Loss: 0.4843853547215204\n",
      "Epoch: 28, Loss: 0.4798759544223269\n",
      "Epoch: 29, Loss: 0.475493221448256\n",
      "Epoch: 30, Loss: 0.4712321055022995\n",
      "Epoch: 31, Loss: 0.46708779628863434\n",
      "Epoch: 32, Loss: 0.46305571146856706\n",
      "Epoch: 33, Loss: 0.4591314851434337\n",
      "Epoch: 34, Loss: 0.4553109568599853\n",
      "Epoch: 35, Loss: 0.4515901611300358\n",
      "Epoch: 36, Loss: 0.4479653174531781\n",
      "Epoch: 37, Loss: 0.4444328208290778\n",
      "Epoch: 38, Loss: 0.44098923274411717\n",
      "Epoch: 39, Loss: 0.43763127261590395\n",
      "Epoch: 40, Loss: 0.43435580967829984\n",
      "Epoch: 41, Loss: 0.43115985528908857\n",
      "Epoch: 42, Loss: 0.4280405556421426\n",
      "Epoch: 43, Loss: 0.42499518486590465\n",
      "Epoch: 44, Loss: 0.42202113849013806\n",
      "Epoch: 45, Loss: 0.4191159272631756\n",
      "Epoch: 46, Loss: 0.41627717130228775\n",
      "Epoch: 47, Loss: 0.4135025945602624\n",
      "Epoch: 48, Loss: 0.4107900195918291\n",
      "Epoch: 49, Loss: 0.4081373626041437\n",
      "Epoch: 50, Loss: 0.4055426287761608\n",
      "Epoch: 51, Loss: 0.40300390783236334\n",
      "Epoch: 52, Loss: 0.40051936985695086\n",
      "Epoch: 53, Loss: 0.39808726133524086\n",
      "Epoch: 54, Loss: 0.3957059014096707\n",
      "Epoch: 55, Loss: 0.393373678338415\n",
      "Epoch: 56, Loss: 0.39108904614524975\n",
      "Epoch: 57, Loss: 0.3888505214498853\n",
      "Epoch: 58, Loss: 0.38665668046857005\n",
      "Epoch: 59, Loss: 0.38450615617531764\n",
      "Epoch: 60, Loss: 0.3823976356146429\n",
      "Epoch: 61, Loss: 0.3803298573572032\n",
      "Epoch: 62, Loss: 0.37830160909022353\n",
      "Epoch: 63, Loss: 0.3763117253350468\n",
      "Epoch: 64, Loss: 0.37435908528459416\n",
      "Epoch: 65, Loss: 0.3724426107539294\n",
      "Epoch: 66, Loss: 0.3705612642375242\n",
      "Epoch: 67, Loss: 0.3687140470671888\n",
      "Epoch: 68, Loss: 0.3668999976649883\n",
      "Epoch: 69, Loss: 0.365118189885798\n",
      "Epoch: 70, Loss: 0.36336773144446344\n",
      "Epoch: 71, Loss: 0.361647762422832\n",
      "Epoch: 72, Loss: 0.3599574538521938\n",
      "Epoch: 73, Loss: 0.35829600636694287\n",
      "Epoch: 74, Loss: 0.3566626489255064\n",
      "Epoch: 75, Loss: 0.3550566375948316\n",
      "Epoch: 76, Loss: 0.3534772543949301\n",
      "Epoch: 77, Loss: 0.3519238062001929\n",
      "Epoch: 78, Loss: 0.35039562369437594\n",
      "Epoch: 79, Loss: 0.34889206037634274\n",
      "Epoch: 80, Loss: 0.3474124916138161\n",
      "Epoch: 81, Loss: 0.3459563137425566\n",
      "Epoch: 82, Loss: 0.34452294320853066\n",
      "Epoch: 83, Loss: 0.3431118157507762\n",
      "Epoch: 84, Loss: 0.341722385622805\n",
      "Epoch: 85, Loss: 0.34035412485050426\n",
      "Epoch: 86, Loss: 0.3390065225246207\n",
      "Epoch: 87, Loss: 0.3376790841260155\n",
      "Epoch: 88, Loss: 0.33637133088198556\n",
      "Epoch: 89, Loss: 0.3350827991520424\n",
      "Epoch: 90, Loss: 0.3338130398416298\n",
      "Epoch: 91, Loss: 0.3325616178423496\n",
      "Epoch: 92, Loss: 0.3313281114973425\n",
      "Epoch: 93, Loss: 0.33011211209054914\n",
      "Epoch: 94, Loss: 0.32891322335864637\n",
      "Epoch: 95, Loss: 0.32773106102451965\n",
      "Epoch: 96, Loss: 0.3265652523511987\n",
      "Epoch: 97, Loss: 0.3254154357152379\n",
      "Epoch: 98, Loss: 0.3242812601985832\n",
      "Epoch: 99, Loss: 0.3231623851980168\n",
      "Epoch: 100, Loss: 0.3220584800513202\n",
      "Epoch: 101, Loss: 0.3209692236793451\n",
      "Epoch: 102, Loss: 0.3198943042432213\n",
      "Epoch: 103, Loss: 0.3188334188159766\n",
      "Epoch: 104, Loss: 0.31778627306787727\n",
      "Epoch: 105, Loss: 0.31675258096483944\n",
      "Epoch: 106, Loss: 0.3157320644792916\n",
      "Epoch: 107, Loss: 0.31472445331290355\n",
      "Epoch: 108, Loss: 0.31372948463062766\n",
      "Epoch: 109, Loss: 0.31274690280552475\n",
      "Epoch: 110, Loss: 0.3117764591738777\n",
      "Epoch: 111, Loss: 0.3108179118001181\n",
      "Epoch: 112, Loss: 0.30987102525111826\n",
      "Epoch: 113, Loss: 0.30893557037942215\n",
      "Epoch: 114, Loss: 0.30801132411500953\n",
      "Epoch: 115, Loss: 0.3070980692652126\n",
      "Epoch: 116, Loss: 0.3061955943224162\n",
      "Epoch: 117, Loss: 0.3053036932791991\n",
      "Epoch: 118, Loss: 0.30442216545058365\n",
      "Epoch: 119, Loss: 0.30355081530308325\n",
      "Epoch: 120, Loss: 0.3026894522902491\n",
      "Epoch: 121, Loss: 0.30183789069443273\n",
      "Epoch: 122, Loss: 0.30099594947449626\n",
      "Epoch: 123, Loss: 0.30016345211921286\n",
      "Epoch: 124, Loss: 0.2993402265061149\n",
      "Epoch: 125, Loss: 0.2985261047655564\n",
      "Epoch: 126, Loss: 0.29772092314976967\n",
      "Epoch: 127, Loss: 0.29692452190670515\n",
      "Epoch: 128, Loss: 0.2961367451584542\n",
      "Epoch: 129, Loss: 0.29535744078406256\n",
      "Epoch: 130, Loss: 0.2945864603065548\n",
      "Epoch: 131, Loss: 0.29382365878399197\n",
      "Epoch: 132, Loss: 0.2930688947044017\n",
      "Epoch: 133, Loss: 0.29232202988441797\n",
      "Epoch: 134, Loss: 0.29158292937148383\n",
      "Epoch: 135, Loss: 0.29085146134946926\n",
      "Epoch: 136, Loss: 0.2901274970475708\n",
      "Epoch: 137, Loss: 0.2894109106523582\n",
      "Epoch: 138, Loss: 0.28870157922284523\n",
      "Epoch: 139, Loss: 0.28799938260846414\n",
      "Epoch: 140, Loss: 0.2873042033698289\n",
      "Epoch: 141, Loss: 0.286615926702179\n",
      "Epoch: 142, Loss: 0.28593444036139876\n",
      "Epoch: 143, Loss: 0.28525963459251213\n",
      "Epoch: 144, Loss: 0.2845914020605581\n",
      "Epoch: 145, Loss: 0.2839296377837548\n",
      "Epoch: 146, Loss: 0.28327423906886473\n",
      "Epoch: 147, Loss: 0.2826251054486786\n",
      "Epoch: 148, Loss: 0.2819821386215361\n",
      "Epoch: 149, Loss: 0.28134524239280845\n",
      "Epoch: 150, Loss: 0.2807143226182681\n",
      "Epoch: 151, Loss: 0.2800892871492765\n",
      "Epoch: 152, Loss: 0.279470045779721\n",
      "Epoch: 153, Loss: 0.27885651019463836\n",
      "Epoch: 154, Loss: 0.27824859392046125\n",
      "Epoch: 155, Loss: 0.2776462122768294\n",
      "Epoch: 156, Loss: 0.277049282329908\n",
      "Epoch: 157, Loss: 0.27645772284715986\n",
      "Epoch: 158, Loss: 0.27587145425351783\n",
      "Epoch: 159, Loss: 0.2752903985889074\n",
      "Epoch: 160, Loss: 0.27471447946707345\n",
      "Epoch: 161, Loss: 0.27414362203566095\n",
      "Epoch: 162, Loss: 0.2735777529375099\n",
      "Epoch: 163, Loss: 0.27301680027311814\n",
      "Epoch: 164, Loss: 0.27246069356423314\n",
      "Epoch: 165, Loss: 0.2719093637185336\n",
      "Epoch: 166, Loss: 0.27136274299536156\n",
      "Epoch: 167, Loss: 0.2708207649724709\n",
      "Epoch: 168, Loss: 0.27028336451375534\n",
      "Epoch: 169, Loss: 0.2697504777379241\n",
      "Epoch: 170, Loss: 0.26922204198809246\n",
      "Epoch: 171, Loss: 0.26869799580225523\n",
      "Epoch: 172, Loss: 0.26817827888461576\n",
      "Epoch: 173, Loss: 0.2676628320777394\n",
      "Epoch: 174, Loss: 0.2671515973355052\n",
      "Epoch: 175, Loss: 0.26664451769682923\n",
      "Epoch: 176, Loss: 0.26614153726013373\n",
      "Epoch: 177, Loss: 0.2656426011585379\n",
      "Epoch: 178, Loss: 0.2651476555357458\n",
      "Epoch: 179, Loss: 0.2646566475226104\n",
      "Epoch: 180, Loss: 0.26416952521434967\n",
      "Epoch: 181, Loss: 0.2636862376483953\n",
      "Epoch: 182, Loss: 0.26320673478285306\n",
      "Epoch: 183, Loss: 0.262730967475555\n",
      "Epoch: 184, Loss: 0.2622588874636852\n",
      "Epoch: 185, Loss: 0.26179044734396006\n",
      "Epoch: 186, Loss: 0.26132560055334697\n",
      "Epoch: 187, Loss: 0.26086430135030225\n",
      "Epoch: 188, Loss: 0.2604065047965146\n",
      "Epoch: 189, Loss: 0.25995216673913635\n",
      "Epoch: 190, Loss: 0.2595012437934883\n",
      "Epoch: 191, Loss: 0.2590536933262233\n",
      "Epoch: 192, Loss: 0.25860947343893426\n",
      "Epoch: 193, Loss: 0.25816854295219366\n",
      "Epoch: 194, Loss: 0.25773086139000995\n",
      "Epoch: 195, Loss: 0.25729638896469004\n",
      "Epoch: 196, Loss: 0.25686508656209417\n",
      "Epoch: 197, Loss: 0.2564369157272719\n",
      "Epoch: 198, Loss: 0.2560118386504675\n",
      "Epoch: 199, Loss: 0.25558981815348425\n",
      "Epoch: 200, Loss: 0.2551708176763962\n",
      "Epoch: 201, Loss: 0.2547548012645983\n",
      "Epoch: 202, Loss: 0.2543417335561829\n",
      "Epoch: 203, Loss: 0.2539315797696354\n",
      "Epoch: 204, Loss: 0.25352430569183787\n",
      "Epoch: 205, Loss: 0.25311987766637295\n",
      "Epoch: 206, Loss: 0.25271826258211794\n",
      "Epoch: 207, Loss: 0.2523194278621223\n",
      "Epoch: 208, Loss: 0.2519233414527593\n",
      "Epoch: 209, Loss: 0.25152997181314396\n",
      "Epoch: 210, Loss: 0.25113928790481027\n",
      "Epoch: 211, Loss: 0.25075125918164015\n",
      "Epoch: 212, Loss: 0.2503658555800367\n",
      "Epoch: 213, Loss: 0.249983047509335\n",
      "Epoch: 214, Loss: 0.24960280584244382\n",
      "Epoch: 215, Loss: 0.24922510190671218\n",
      "Epoch: 216, Loss: 0.24884990747501373\n",
      "Epoch: 217, Loss: 0.2484771947570432\n",
      "Epoch: 218, Loss: 0.24810693639081954\n",
      "Epoch: 219, Loss: 0.24773910543438973\n",
      "Epoch: 220, Loss: 0.2473736753577268\n",
      "Epoch: 221, Loss: 0.24701062003481897\n",
      "Epoch: 222, Loss: 0.24664991373594236\n",
      "Epoch: 223, Loss: 0.24629153112011376\n",
      "Epoch: 224, Loss: 0.24593544722771823\n",
      "Epoch: 225, Loss: 0.24558163747330658\n",
      "Epoch: 226, Loss: 0.2452300776385585\n",
      "Epoch: 227, Loss: 0.24488074386540667\n",
      "Epoch: 228, Loss: 0.24453361264931792\n",
      "Epoch: 229, Loss: 0.24418866083272686\n",
      "Epoch: 230, Loss: 0.24384586559861807\n",
      "Epoch: 231, Loss: 0.24350520446425364\n",
      "Epoch: 232, Loss: 0.2431666552750406\n",
      "Epoch: 233, Loss: 0.24283019619853688\n",
      "Epoch: 234, Loss: 0.2424958057185899\n",
      "Epoch: 235, Loss: 0.24216346262960617\n",
      "Epoch: 236, Loss: 0.2418331460309474\n",
      "Epoch: 237, Loss: 0.24150483532145053\n",
      "Epoch: 238, Loss: 0.2411785101940678\n",
      "Epoch: 239, Loss: 0.24085415063062496\n",
      "Epoch: 240, Loss: 0.2405317368966931\n",
      "Epoch: 241, Loss: 0.24021124953657277\n",
      "Epoch: 242, Loss: 0.2398926693683862\n",
      "Epoch: 243, Loss: 0.23957597747927617\n",
      "Epoch: 244, Loss: 0.23926115522070754\n",
      "Epoch: 245, Loss: 0.23894818420387012\n",
      "Epoch: 246, Loss: 0.23863704629517976\n",
      "Epoch: 247, Loss: 0.23832772361187485\n",
      "Epoch: 248, Loss: 0.23802019851770703\n",
      "Epoch: 249, Loss: 0.23771445361872232\n",
      "Epoch: 250, Loss: 0.2374104717591319\n",
      "Epoch: 251, Loss: 0.23710823601726935\n",
      "Epoch: 252, Loss: 0.23680772970163266\n",
      "Epoch: 253, Loss: 0.23650893634700884\n",
      "Epoch: 254, Loss: 0.2362118397106794\n",
      "Epoch: 255, Loss: 0.23591642376870428\n",
      "Epoch: 256, Loss: 0.23562267271228293\n",
      "Epoch: 257, Loss: 0.23533057094419005\n",
      "Epoch: 258, Loss: 0.23504010307528486\n",
      "Epoch: 259, Loss: 0.23475125392109195\n",
      "Epoch: 260, Loss: 0.2344640084984517\n",
      "Epoch: 261, Loss: 0.23417835202223874\n",
      "Epoch: 262, Loss: 0.23389426990214784\n",
      "Epoch: 263, Loss: 0.23361174773954352\n",
      "Epoch: 264, Loss: 0.23333077132437463\n",
      "Epoch: 265, Loss: 0.2330513266321499\n",
      "Epoch: 266, Loss: 0.23277339982097467\n",
      "Epoch: 267, Loss: 0.23249697722864668\n",
      "Epoch: 268, Loss: 0.23222204536981\n",
      "Epoch: 269, Loss: 0.23194859093316492\n",
      "Epoch: 270, Loss: 0.23167660077873412\n",
      "Epoch: 271, Loss: 0.23140606193518182\n",
      "Epoch: 272, Loss: 0.23113696159718664\n",
      "Epoch: 273, Loss: 0.2308692871228654\n",
      "Epoch: 274, Loss: 0.23060302603124794\n",
      "Epoch: 275, Loss: 0.23033816599980125\n",
      "Epoch: 276, Loss: 0.2300746948620012\n",
      "Epoch: 277, Loss: 0.22981260060495237\n",
      "Epoch: 278, Loss: 0.22955187136705374\n",
      "Epoch: 279, Loss: 0.22929249543570912\n",
      "Epoch: 280, Loss: 0.22903446124508234\n",
      "Epoch: 281, Loss: 0.22877775737389527\n",
      "Epoch: 282, Loss: 0.2285223725432683\n",
      "Epoch: 283, Loss: 0.22826829561460205\n",
      "Epoch: 284, Loss: 0.22801551558749955\n",
      "Epoch: 285, Loss: 0.22776402159772818\n",
      "Epoch: 286, Loss: 0.2275138029152203\n",
      "Epoch: 287, Loss: 0.22726484894211152\n",
      "Epoch: 288, Loss: 0.22701714921081645\n",
      "Epoch: 289, Loss: 0.22677069338214098\n",
      "Epoch: 290, Loss: 0.22652547124342948\n",
      "Epoch: 291, Loss: 0.22628147270674734\n",
      "Epoch: 292, Loss: 0.22603868780709752\n",
      "Epoch: 293, Loss: 0.2257971067006705\n",
      "Epoch: 294, Loss: 0.22555671966312693\n",
      "Epoch: 295, Loss: 0.22531751708791226\n",
      "Epoch: 296, Loss: 0.22507948948460282\n",
      "Epoch: 297, Loss: 0.22484262747728276\n",
      "Epoch: 298, Loss: 0.2246069218029508\n",
      "Epoch: 299, Loss: 0.22437236330995694\n",
      "Epoch: 300, Loss: 0.2241389429564675\n",
      "Epoch: 301, Loss: 0.2239066518089592\n",
      "Epoch: 302, Loss: 0.22367548104074025\n",
      "Epoch: 303, Loss: 0.22344542193049946\n",
      "Epoch: 304, Loss: 0.22321646586088123\n",
      "Epoch: 305, Loss: 0.22298860431708736\n",
      "Epoch: 306, Loss: 0.22276182888550405\n",
      "Epoch: 307, Loss: 0.22253613125235439\n",
      "Epoch: 308, Loss: 0.22231150320237508\n",
      "Epoch: 309, Loss: 0.2220879366175179\n",
      "Epoch: 310, Loss: 0.22186542347567423\n",
      "Epoch: 311, Loss: 0.22164395584942345\n",
      "Epoch: 312, Loss: 0.22142352590480366\n",
      "Epoch: 313, Loss: 0.221204125900105\n",
      "Epoch: 314, Loss: 0.22098574818468517\n",
      "Epoch: 315, Loss: 0.22076838519780584\n",
      "Epoch: 316, Loss: 0.22055202946749095\n",
      "Epoch: 317, Loss: 0.22033667360940515\n",
      "Epoch: 318, Loss: 0.22012231032575316\n",
      "Epoch: 319, Loss: 0.21990893240419854\n",
      "Epoch: 320, Loss: 0.21969653271680253\n",
      "Epoch: 321, Loss: 0.2194851042189822\n",
      "Epoch: 322, Loss: 0.2192746399484868\n",
      "Epoch: 323, Loss: 0.21906513302439387\n",
      "Epoch: 324, Loss: 0.21885657664612257\n",
      "Epoch: 325, Loss: 0.2186489640924654\n",
      "Epoch: 326, Loss: 0.21844228872063745\n",
      "Epoch: 327, Loss: 0.21823654396534317\n",
      "Epoch: 328, Loss: 0.21803172333785967\n",
      "Epoch: 329, Loss: 0.217827820425137\n",
      "Epoch: 330, Loss: 0.21762482888891502\n",
      "Epoch: 331, Loss: 0.21742274246485616\n",
      "Epoch: 332, Loss: 0.217221554961694\n",
      "Epoch: 333, Loss: 0.21702126026039784\n",
      "Epoch: 334, Loss: 0.21682185231335185\n",
      "Epoch: 335, Loss: 0.2166233251435503\n",
      "Epoch: 336, Loss: 0.2164256728438068\n",
      "Epoch: 337, Loss: 0.2162288895759785\n",
      "Epoch: 338, Loss: 0.2160329695702046\n",
      "Epoch: 339, Loss: 0.21583790712415898\n",
      "Epoch: 340, Loss: 0.2156436966023167\n",
      "Epoch: 341, Loss: 0.21545033243523426\n",
      "Epoch: 342, Loss: 0.21525780911884299\n",
      "Epoch: 343, Loss: 0.21506612121375648\n",
      "Epoch: 344, Loss: 0.21487526334458992\n",
      "Epoch: 345, Loss: 0.21468523019929342\n",
      "Epoch: 346, Loss: 0.2144960165284973\n",
      "Epoch: 347, Loss: 0.21430761714486998\n",
      "Epoch: 348, Loss: 0.21412002692248838\n",
      "Epoch: 349, Loss: 0.2139332407962199\n",
      "Epoch: 350, Loss: 0.21374725376111733\n",
      "Epoch: 351, Loss: 0.2135620608718241\n",
      "Epoch: 352, Loss: 0.2133776572419925\n",
      "Epoch: 353, Loss: 0.21319403804371242\n",
      "Epoch: 354, Loss: 0.21301119850695174\n",
      "Epoch: 355, Loss: 0.21282913391900782\n",
      "Epoch: 356, Loss: 0.2126478396239698\n",
      "Epoch: 357, Loss: 0.21246731102219182\n",
      "Epoch: 358, Loss: 0.2122875435697771\n",
      "Epoch: 359, Loss: 0.2121085327780722\n",
      "Epoch: 360, Loss: 0.21193027421317184\n",
      "Epoch: 361, Loss: 0.2117527634954343\n",
      "Epoch: 362, Loss: 0.2115759962990063\n",
      "Epoch: 363, Loss: 0.21139996835135866\n",
      "Epoch: 364, Loss: 0.21122467543283138\n",
      "Epoch: 365, Loss: 0.21105011337618848\n",
      "Epoch: 366, Loss: 0.21087627806618295\n",
      "Epoch: 367, Loss: 0.21070316543913076\n",
      "Epoch: 368, Loss: 0.21053077148249472\n",
      "Epoch: 369, Loss: 0.21035909223447752\n",
      "Epoch: 370, Loss: 0.21018812378362392\n",
      "Epoch: 371, Loss: 0.2100178622684324\n",
      "Epoch: 372, Loss: 0.2098483038769755\n",
      "Epoch: 373, Loss: 0.20967944484652967\n",
      "Epoch: 374, Loss: 0.2095112814632134\n",
      "Epoch: 375, Loss: 0.2093438100616344\n",
      "Epoch: 376, Loss: 0.20917702702454594\n",
      "Epoch: 377, Loss: 0.20901092878251087\n",
      "Epoch: 378, Loss: 0.20884551181357536\n",
      "Epoch: 379, Loss: 0.20868077264295015\n",
      "Epoch: 380, Loss: 0.20851670784270088\n",
      "Epoch: 381, Loss: 0.2083533140314461\n",
      "Epoch: 382, Loss: 0.2081905878740649\n",
      "Epoch: 383, Loss: 0.20802852608141087\n",
      "Epoch: 384, Loss: 0.207867125410036\n",
      "Epoch: 385, Loss: 0.2077063826619216\n",
      "Epoch: 386, Loss: 0.20754629468421776\n",
      "Epoch: 387, Loss: 0.20738685836899065\n",
      "Epoch: 388, Loss: 0.2072280706529781\n",
      "Epoch: 389, Loss: 0.20706992851735245\n",
      "Epoch: 390, Loss: 0.20691242898749207\n",
      "Epoch: 391, Loss: 0.2067555691327601\n",
      "Epoch: 392, Loss: 0.20659934606629152\n",
      "Epoch: 393, Loss: 0.20644375694478737\n",
      "Epoch: 394, Loss: 0.2062887989683173\n",
      "Epoch: 395, Loss: 0.20613446938012955\n",
      "Epoch: 396, Loss: 0.2059807654664687\n",
      "Epoch: 397, Loss: 0.20582768455640071\n",
      "Epoch: 398, Loss: 0.20567522402164642\n",
      "Epoch: 399, Loss: 0.20552338127642175\n",
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.5617862179295732\n",
      "Epoch: 1, Loss: 0.4206871862203998\n",
      "Epoch: 2, Loss: 0.35576372214133806\n",
      "Epoch: 3, Loss: 0.3183050971825107\n",
      "Epoch: 4, Loss: 0.29317055638557893\n",
      "Epoch: 5, Loss: 0.2754553690415158\n",
      "Epoch: 6, Loss: 0.26185707392248775\n",
      "Epoch: 7, Loss: 0.2517095073807133\n",
      "Epoch: 8, Loss: 0.2430066896454463\n",
      "Epoch: 9, Loss: 0.2359802038867639\n",
      "Epoch: 10, Loss: 0.2302995243289645\n",
      "Epoch: 11, Loss: 0.22498101630719508\n",
      "Epoch: 12, Loss: 0.22094207566353036\n",
      "Epoch: 13, Loss: 0.21720286127110816\n",
      "Epoch: 14, Loss: 0.21400467887320296\n",
      "Epoch: 15, Loss: 0.21104428563780803\n",
      "Epoch: 16, Loss: 0.2084030400238119\n",
      "Epoch: 17, Loss: 0.2060928472845162\n",
      "Epoch: 18, Loss: 0.20397167779106976\n",
      "Epoch: 19, Loss: 0.20186028635278078\n",
      "Epoch: 20, Loss: 0.2003786379957937\n",
      "Epoch: 21, Loss: 0.19883465954003302\n",
      "Epoch: 22, Loss: 0.19732278014808233\n",
      "Epoch: 23, Loss: 0.19621452096381645\n",
      "Epoch: 24, Loss: 0.1948535947361444\n",
      "Epoch: 25, Loss: 0.19383836417446093\n",
      "Epoch: 26, Loss: 0.19266214863174938\n",
      "Epoch: 27, Loss: 0.19166672597266726\n",
      "Epoch: 28, Loss: 0.19081228614950416\n",
      "Epoch: 29, Loss: 0.19002242437566008\n",
      "Epoch: 30, Loss: 0.18924414037673223\n",
      "Epoch: 31, Loss: 0.18859167446768482\n",
      "Epoch: 32, Loss: 0.18782116262832993\n",
      "Epoch: 33, Loss: 0.18725744996077384\n",
      "Epoch: 34, Loss: 0.18678981484263985\n",
      "Epoch: 35, Loss: 0.18623043007666001\n",
      "Epoch: 36, Loss: 0.18562538246090407\n",
      "Epoch: 37, Loss: 0.18507939804044757\n",
      "Epoch: 38, Loss: 0.18474206105646598\n",
      "Epoch: 39, Loss: 0.18430956037190155\n",
      "Epoch: 40, Loss: 0.18377702176417232\n",
      "Epoch: 41, Loss: 0.18346733910651833\n",
      "Epoch: 42, Loss: 0.18323957060472387\n",
      "Epoch: 43, Loss: 0.1829354678556678\n",
      "Epoch: 44, Loss: 0.1825514888496216\n",
      "Epoch: 45, Loss: 0.1820438616089029\n",
      "Epoch: 46, Loss: 0.18187471012391607\n",
      "Epoch: 47, Loss: 0.18165524238415603\n",
      "Epoch: 48, Loss: 0.18148246718926642\n",
      "Epoch: 49, Loss: 0.18063406456711797\n",
      "Epoch: 50, Loss: 0.18083704210952828\n",
      "Stopping early due to increase in average loss.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for GD: 46.41500282287598 seconds\n",
      "Training time for SGD: 11.212540864944458 seconds\n",
      "Accuracy for GD: 0.916010498687664\n",
      "Accuracy for SGD: 0.9146981627296588\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "alpha = 1\n",
    "lr = 1\n",
    "epochs = 400\n",
    "\n",
    "# Train using GD\n",
    "w_gd, b_gd, losses_gd, time_gd = train_batch_gd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "accuracy_gd = np.mean(predict(X_test.to_numpy(), w_gd, b_gd) == y_test.to_numpy())\n",
    "\n",
    "alpha = 0.1\n",
    "lr = 0.01\n",
    "# Train using SGD\n",
    "w_sgd, b_sgd, losses_sgd, time_sgd = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "accuracy_sgd = np.mean(predict(X_test.to_numpy(), w_sgd, b_sgd) == y_test.to_numpy())\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(losses_gd, label='GD')\n",
    "plt.plot(losses_sgd, label='SGD')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the training times\n",
    "print(f'Training time for GD: {time_gd} seconds')\n",
    "print(f'Training time for SGD: {time_sgd} seconds')\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Accuracy for GD: {accuracy_gd}')\n",
    "print(f'Accuracy for SGD: {accuracy_sgd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD is dramatically faster than BGD and final accuracy is better than BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.5602125184282873\n",
      "Epoch: 1, Loss: 0.42069362460463394\n",
      "Epoch: 2, Loss: 0.35582478271206397\n",
      "Epoch: 3, Loss: 0.31819412598494407\n",
      "Epoch: 4, Loss: 0.293058846597446\n",
      "Epoch: 5, Loss: 0.2753759335829414\n",
      "Epoch: 6, Loss: 0.2620876672246737\n",
      "Epoch: 7, Loss: 0.2510275822956573\n",
      "Epoch: 8, Loss: 0.24302897206866766\n",
      "Epoch: 9, Loss: 0.2361331645560287\n",
      "Epoch: 10, Loss: 0.2302349244823138\n",
      "Epoch: 11, Loss: 0.22520134991067645\n",
      "Epoch: 12, Loss: 0.2210012697645134\n",
      "Epoch: 13, Loss: 0.2172919164422643\n",
      "Epoch: 14, Loss: 0.21340333206130196\n",
      "Epoch: 15, Loss: 0.21092372748648186\n",
      "Epoch: 16, Loss: 0.20847808332496967\n",
      "Epoch: 17, Loss: 0.20573412702986377\n",
      "Epoch: 18, Loss: 0.20355040948225037\n",
      "Epoch: 19, Loss: 0.20140832951891185\n",
      "Epoch: 20, Loss: 0.20054669703459096\n",
      "Epoch: 21, Loss: 0.19853976874920867\n",
      "Epoch: 22, Loss: 0.19727777190064194\n",
      "Epoch: 23, Loss: 0.19608967065540975\n",
      "Epoch: 24, Loss: 0.194088977740817\n",
      "Epoch: 25, Loss: 0.19353171616193351\n",
      "Epoch: 26, Loss: 0.19231278837084895\n",
      "Epoch: 27, Loss: 0.191241210822309\n",
      "Epoch: 28, Loss: 0.19098614726650207\n",
      "Epoch: 29, Loss: 0.1898834012604412\n",
      "Epoch: 30, Loss: 0.18913732827245322\n",
      "Epoch: 31, Loss: 0.18865212299401535\n",
      "Epoch: 32, Loss: 0.1878173521461611\n",
      "Epoch: 33, Loss: 0.187268764583714\n",
      "Epoch: 34, Loss: 0.1866396292305566\n",
      "Epoch: 35, Loss: 0.1858751721324011\n",
      "Epoch: 36, Loss: 0.18565194945186292\n",
      "Epoch: 37, Loss: 0.1850930802879033\n",
      "Epoch: 38, Loss: 0.1847277548119487\n",
      "Epoch: 39, Loss: 0.18423619485482165\n",
      "Epoch: 40, Loss: 0.18377874631253413\n",
      "Epoch: 41, Loss: 0.18352903577255275\n",
      "Epoch: 42, Loss: 0.18323149332036037\n",
      "Epoch: 43, Loss: 0.1827838617984922\n",
      "Epoch: 44, Loss: 0.1826119409637927\n",
      "Epoch: 45, Loss: 0.18194968693887167\n",
      "Epoch: 46, Loss: 0.18202112778524304\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.30215394819449803\n",
      "Epoch: 1, Loss: 0.2018782130998305\n",
      "Epoch: 2, Loss: 0.18289499586997987\n",
      "Epoch: 3, Loss: 0.17550348431829418\n",
      "Epoch: 4, Loss: 0.1713257081227317\n",
      "Epoch: 5, Loss: 0.17055094758004394\n",
      "Epoch: 6, Loss: 0.1702790679001398\n",
      "Epoch: 7, Loss: 0.1684720966176725\n",
      "Epoch: 8, Loss: 0.16918915440502771\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.09432631278137805\n",
      "Epoch: 1, Loss: 4.185038165308182\n",
      "Stopping early due to increase in average loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/b29xgfjn237gd2tgvs743q8m0000gn/T/ipykernel_47040/2022709499.py:24: RuntimeWarning: overflow encountered in multiply\n",
      "  dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w*w)/N)\n",
      "/var/folders/sj/b29xgfjn237gd2tgvs743q8m0000gn/T/ipykernel_47040/2022709499.py:24: RuntimeWarning: overflow encountered in multiply\n",
      "  dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w*w)/N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: nan\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.527017192760679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/b29xgfjn237gd2tgvs743q8m0000gn/T/ipykernel_47040/2022709499.py:24: RuntimeWarning: overflow encountered in multiply\n",
      "  dw = x * (y-sigmoid(np.dot(w.T,x)+b)) - ((alpha*w*w)/N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: nan\n",
      "Stopping early due to increase in average loss.\n"
     ]
    }
   ],
   "source": [
    "# Now we will test different learning rates for the SGD model\n",
    "\n",
    "lr_values = [0.01, 0.1, 1, 10, 100]\n",
    "all_losses = []\n",
    "all_accuracies = []\n",
    "\n",
    "for lr in lr_values:\n",
    "    w, b = initialize_weights(X_train.shape[1])\n",
    "    alpha = 0.1\n",
    "    epochs = 200\n",
    "    w, b, losses, training_time = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "    test_loss = test(X_test, y_test, w, b)\n",
    "    all_losses.append(test_loss)\n",
    "    y_pred = predict(X_test.to_numpy(), w, b)\n",
    "    test_accuracy = np.mean(y_pred == y_test.to_numpy())\n",
    "    all_accuracies.append(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20616185361260939, 0.21168949356132077, 10.926703017541298, nan, nan]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9186351706036745,\n",
       " 0.9120734908136483,\n",
       " 0.4068241469816273,\n",
       " 0.4068241469816273,\n",
       " 0.4068241469816273]"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accuracies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's plot 0.01 and 0.1 learning rates for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.5612163401973288\n",
      "Epoch: 1, Loss: 0.42132398813972255\n",
      "Epoch: 2, Loss: 0.35614951987995513\n",
      "Epoch: 3, Loss: 0.31814210719323\n",
      "Epoch: 4, Loss: 0.29287860587404585\n",
      "Epoch: 5, Loss: 0.2753836198128397\n",
      "Epoch: 6, Loss: 0.2615905939077615\n",
      "Epoch: 7, Loss: 0.2516580478091464\n",
      "Epoch: 8, Loss: 0.24298347313421706\n",
      "Epoch: 9, Loss: 0.2360653256299324\n",
      "Epoch: 10, Loss: 0.2303481973221735\n",
      "Epoch: 11, Loss: 0.2251464616761629\n",
      "Epoch: 12, Loss: 0.22070869336143117\n",
      "Epoch: 13, Loss: 0.21720149767029687\n",
      "Epoch: 14, Loss: 0.21388955173791963\n",
      "Epoch: 15, Loss: 0.2107894359466435\n",
      "Epoch: 16, Loss: 0.20842610862187902\n",
      "Epoch: 17, Loss: 0.20604408707413605\n",
      "Epoch: 18, Loss: 0.20374795686531608\n",
      "Epoch: 19, Loss: 0.20215114660533282\n",
      "Epoch: 20, Loss: 0.20038366122450596\n",
      "Epoch: 21, Loss: 0.198878400091156\n",
      "Epoch: 22, Loss: 0.1974287696585895\n",
      "Epoch: 23, Loss: 0.1959150874043499\n",
      "Epoch: 24, Loss: 0.19473206209815433\n",
      "Epoch: 25, Loss: 0.19363556037341395\n",
      "Epoch: 26, Loss: 0.19279410433243047\n",
      "Epoch: 27, Loss: 0.1917524308913465\n",
      "Epoch: 28, Loss: 0.1906714878359803\n",
      "Epoch: 29, Loss: 0.1898219086159774\n",
      "Epoch: 30, Loss: 0.18918226059859314\n",
      "Epoch: 31, Loss: 0.1884044181363633\n",
      "Epoch: 32, Loss: 0.18787244482829377\n",
      "Epoch: 33, Loss: 0.18732018728290992\n",
      "Epoch: 34, Loss: 0.18597957501121773\n",
      "Epoch: 35, Loss: 0.18618801547851813\n",
      "Stopping early due to increase in average loss.\n",
      "Initial Loss: 6.527017192760679\n",
      "Epoch: 0, Loss: 0.3037085632382837\n",
      "Epoch: 1, Loss: 0.20056215025372603\n",
      "Epoch: 2, Loss: 0.18195218718559095\n",
      "Epoch: 3, Loss: 0.17544277971279412\n",
      "Epoch: 4, Loss: 0.17107215757374997\n",
      "Epoch: 5, Loss: 0.1694097255358863\n",
      "Epoch: 6, Loss: 0.16950933733916485\n",
      "Stopping early due to increase in average loss.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for SGD-lr=0.01: 7.573582172393799 seconds\n",
      "Training time for SGD-lr=0.1: 1.41725492477417 seconds\n",
      "Accuracy for SGD-lr=0.01: 0.9173228346456693\n",
      "Accuracy for SGD-lr=0.1: 0.9120734908136483\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_data(data_rice)\n",
    "\n",
    "w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "alpha = 0.1\n",
    "lr = 0.01\n",
    "epochs = 200\n",
    "\n",
    "# Train using GD\n",
    "w_sgd_01, b_sgd_01, losses_sgd_01, time_sgd_01 = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "accuracy_sgd_01 = np.mean(predict(X_test.to_numpy(), w_sgd_01, b_sgd_01) == y_test.to_numpy())\n",
    "\n",
    "alpha = 0.1\n",
    "lr = 0.1\n",
    "# Train using SGD\n",
    "w_sgd, b_sgd, losses_sgd, time_sgd = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "accuracy_sgd = np.mean(predict(X_test.to_numpy(), w_sgd, b_sgd) == y_test.to_numpy())\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(losses_sgd_01, label='SGD-lr=0.01')\n",
    "plt.plot(losses_sgd, label='SGD-lr=0.1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the training times\n",
    "print(f'Training time for SGD-lr=0.01: {time_sgd_01} seconds')\n",
    "print(f'Training time for SGD-lr=0.1: {time_sgd} seconds')\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Accuracy for SGD-lr=0.01: {accuracy_sgd_01}')\n",
    "print(f'Accuracy for SGD-lr=0.1: {accuracy_sgd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 17, 'name': 'Breast Cancer Wisconsin (Diagnostic)', 'repository_url': 'https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic', 'data_url': 'https://archive.ics.uci.edu/static/public/17/data.csv', 'abstract': 'Diagnostic Wisconsin Breast Cancer Database.', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 569, 'num_features': 30, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Diagnosis'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1993, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C5DW2B', 'creators': ['William Wolberg', 'Olvi Mangasarian', 'Nick Street', 'W. Street'], 'intro_paper': {'title': 'Nuclear feature extraction for breast tumor diagnosis', 'authors': 'W. Street, W. Wolberg, O. Mangasarian', 'published_in': 'Electronic imaging', 'year': 1993, 'url': 'https://www.semanticscholar.org/paper/53f0fbb425bc14468eb3bf96b2e1d41ba8087f36', 'doi': '10.1117/12.148698'}, 'additional_info': {'summary': 'Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/\\r\\n\\r\\nSeparating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree.  Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\\r\\n\\r\\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\\r\\n\\r\\nThis database is also available through the UW CS ftp server:\\r\\nftp ftp.cs.wisc.edu\\r\\ncd math-prog/cpo-dataset/machine-learn/WDBC/', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1) ID number\\r\\n2) Diagnosis (M = malignant, B = benign)\\r\\n3-32)\\r\\n\\r\\nTen real-valued features are computed for each cell nucleus:\\r\\n\\r\\n\\ta) radius (mean of distances from center to points on the perimeter)\\r\\n\\tb) texture (standard deviation of gray-scale values)\\r\\n\\tc) perimeter\\r\\n\\td) area\\r\\n\\te) smoothness (local variation in radius lengths)\\r\\n\\tf) compactness (perimeter^2 / area - 1.0)\\r\\n\\tg) concavity (severity of concave portions of the contour)\\r\\n\\th) concave points (number of concave portions of the contour)\\r\\n\\ti) symmetry \\r\\n\\tj) fractal dimension (\"coastline approximation\" - 1)', 'citation': None}}\n",
      "                  name     role         type demographic description units  \\\n",
      "0                   ID       ID  Categorical        None        None  None   \n",
      "1            Diagnosis   Target  Categorical        None        None  None   \n",
      "2              radius1  Feature   Continuous        None        None  None   \n",
      "3             texture1  Feature   Continuous        None        None  None   \n",
      "4           perimeter1  Feature   Continuous        None        None  None   \n",
      "5                area1  Feature   Continuous        None        None  None   \n",
      "6          smoothness1  Feature   Continuous        None        None  None   \n",
      "7         compactness1  Feature   Continuous        None        None  None   \n",
      "8           concavity1  Feature   Continuous        None        None  None   \n",
      "9      concave_points1  Feature   Continuous        None        None  None   \n",
      "10           symmetry1  Feature   Continuous        None        None  None   \n",
      "11  fractal_dimension1  Feature   Continuous        None        None  None   \n",
      "12             radius2  Feature   Continuous        None        None  None   \n",
      "13            texture2  Feature   Continuous        None        None  None   \n",
      "14          perimeter2  Feature   Continuous        None        None  None   \n",
      "15               area2  Feature   Continuous        None        None  None   \n",
      "16         smoothness2  Feature   Continuous        None        None  None   \n",
      "17        compactness2  Feature   Continuous        None        None  None   \n",
      "18          concavity2  Feature   Continuous        None        None  None   \n",
      "19     concave_points2  Feature   Continuous        None        None  None   \n",
      "20           symmetry2  Feature   Continuous        None        None  None   \n",
      "21  fractal_dimension2  Feature   Continuous        None        None  None   \n",
      "22             radius3  Feature   Continuous        None        None  None   \n",
      "23            texture3  Feature   Continuous        None        None  None   \n",
      "24          perimeter3  Feature   Continuous        None        None  None   \n",
      "25               area3  Feature   Continuous        None        None  None   \n",
      "26         smoothness3  Feature   Continuous        None        None  None   \n",
      "27        compactness3  Feature   Continuous        None        None  None   \n",
      "28          concavity3  Feature   Continuous        None        None  None   \n",
      "29     concave_points3  Feature   Continuous        None        None  None   \n",
      "30           symmetry3  Feature   Continuous        None        None  None   \n",
      "31  fractal_dimension3  Feature   Continuous        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(breast_cancer_wisconsin_diagnostic.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
       "0    17.99     10.38      122.80  1001.0      0.11840       0.27760   \n",
       "1    20.57     17.77      132.90  1326.0      0.08474       0.07864   \n",
       "2    19.69     21.25      130.00  1203.0      0.10960       0.15990   \n",
       "3    11.42     20.38       77.58   386.1      0.14250       0.28390   \n",
       "4    20.29     14.34      135.10  1297.0      0.10030       0.13280   \n",
       "\n",
       "   concavity1  concave_points1  symmetry1  fractal_dimension1  ...  texture3  \\\n",
       "0      0.3001          0.14710     0.2419             0.07871  ...     17.33   \n",
       "1      0.0869          0.07017     0.1812             0.05667  ...     23.41   \n",
       "2      0.1974          0.12790     0.2069             0.05999  ...     25.53   \n",
       "3      0.2414          0.10520     0.2597             0.09744  ...     26.50   \n",
       "4      0.1980          0.10430     0.1809             0.05883  ...     16.67   \n",
       "\n",
       "   perimeter3   area3  smoothness3  compactness3  concavity3  concave_points3  \\\n",
       "0      184.60  2019.0       0.1622        0.6656      0.7119           0.2654   \n",
       "1      158.80  1956.0       0.1238        0.1866      0.2416           0.1860   \n",
       "2      152.50  1709.0       0.1444        0.4245      0.4504           0.2430   \n",
       "3       98.87   567.7       0.2098        0.8663      0.6869           0.2575   \n",
       "4      152.20  1575.0       0.1374        0.2050      0.4000           0.1625   \n",
       "\n",
       "   symmetry3  fractal_dimension3  Diagnosis  \n",
       "0     0.4601             0.11890          M  \n",
       "1     0.2750             0.08902          M  \n",
       "2     0.3613             0.08758          M  \n",
       "3     0.6638             0.17300          M  \n",
       "4     0.2364             0.07678          M  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cancer = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cancer[\"Diagnosis\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize the data\n",
    "for column in data_cancer.select_dtypes(include=['number']).columns:  # This ensures only numeric columns are selected (not the target column)\n",
    "    min_col = data_cancer[column].min()\n",
    "    max_col = data_cancer[column].max()\n",
    "\n",
    "    # Avoid division by zero in case max_col equals min_col\n",
    "    if max_col != min_col:\n",
    "        data_cancer[column] = (data_cancer[column] - min_col) / (max_col - min_col)\n",
    "    else:\n",
    "        data_cancer[column] = 0  # Assign 0 to all rows if max_col equals min_col\n",
    "\n",
    "# Replace the Diagnosis column with 0 and 1\n",
    "\n",
    "data_cancer['Diagnosis'] = data_cancer['Diagnosis'].replace('M', 0)\n",
    "data_cancer['Diagnosis'] = data_cancer['Diagnosis'].replace('B', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521037</td>\n",
       "      <td>0.022658</td>\n",
       "      <td>0.545989</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.792037</td>\n",
       "      <td>0.703140</td>\n",
       "      <td>0.731113</td>\n",
       "      <td>0.686364</td>\n",
       "      <td>0.605518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141525</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.450698</td>\n",
       "      <td>0.601136</td>\n",
       "      <td>0.619292</td>\n",
       "      <td>0.568610</td>\n",
       "      <td>0.912027</td>\n",
       "      <td>0.598462</td>\n",
       "      <td>0.418864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643144</td>\n",
       "      <td>0.272574</td>\n",
       "      <td>0.615783</td>\n",
       "      <td>0.501591</td>\n",
       "      <td>0.289880</td>\n",
       "      <td>0.181768</td>\n",
       "      <td>0.203608</td>\n",
       "      <td>0.348757</td>\n",
       "      <td>0.379798</td>\n",
       "      <td>0.141323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.539818</td>\n",
       "      <td>0.435214</td>\n",
       "      <td>0.347553</td>\n",
       "      <td>0.154563</td>\n",
       "      <td>0.192971</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.233590</td>\n",
       "      <td>0.222878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.601496</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.449417</td>\n",
       "      <td>0.514309</td>\n",
       "      <td>0.431017</td>\n",
       "      <td>0.462512</td>\n",
       "      <td>0.635686</td>\n",
       "      <td>0.509596</td>\n",
       "      <td>0.211247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360075</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.374508</td>\n",
       "      <td>0.483590</td>\n",
       "      <td>0.385375</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.403706</td>\n",
       "      <td>0.213433</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210090</td>\n",
       "      <td>0.360839</td>\n",
       "      <td>0.233501</td>\n",
       "      <td>0.102906</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.811361</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.522863</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385928</td>\n",
       "      <td>0.241347</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.915472</td>\n",
       "      <td>0.814012</td>\n",
       "      <td>0.548642</td>\n",
       "      <td>0.884880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.629893</td>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.630986</td>\n",
       "      <td>0.489290</td>\n",
       "      <td>0.430351</td>\n",
       "      <td>0.347893</td>\n",
       "      <td>0.463918</td>\n",
       "      <td>0.518390</td>\n",
       "      <td>0.378283</td>\n",
       "      <td>0.186816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123934</td>\n",
       "      <td>0.506948</td>\n",
       "      <td>0.341575</td>\n",
       "      <td>0.437364</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.319489</td>\n",
       "      <td>0.558419</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.142595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    radius1  texture1  perimeter1     area1  smoothness1  compactness1  \\\n",
       "0  0.521037  0.022658    0.545989  0.363733     0.593753      0.792037   \n",
       "1  0.643144  0.272574    0.615783  0.501591     0.289880      0.181768   \n",
       "2  0.601496  0.390260    0.595743  0.449417     0.514309      0.431017   \n",
       "3  0.210090  0.360839    0.233501  0.102906     0.811321      0.811361   \n",
       "4  0.629893  0.156578    0.630986  0.489290     0.430351      0.347893   \n",
       "\n",
       "   concavity1  concave_points1  symmetry1  fractal_dimension1  ...  texture3  \\\n",
       "0    0.703140         0.731113   0.686364            0.605518  ...  0.141525   \n",
       "1    0.203608         0.348757   0.379798            0.141323  ...  0.303571   \n",
       "2    0.462512         0.635686   0.509596            0.211247  ...  0.360075   \n",
       "3    0.565604         0.522863   0.776263            1.000000  ...  0.385928   \n",
       "4    0.463918         0.518390   0.378283            0.186816  ...  0.123934   \n",
       "\n",
       "   perimeter3     area3  smoothness3  compactness3  concavity3  \\\n",
       "0    0.668310  0.450698     0.601136      0.619292    0.568610   \n",
       "1    0.539818  0.435214     0.347553      0.154563    0.192971   \n",
       "2    0.508442  0.374508     0.483590      0.385375    0.359744   \n",
       "3    0.241347  0.094008     0.915472      0.814012    0.548642   \n",
       "4    0.506948  0.341575     0.437364      0.172415    0.319489   \n",
       "\n",
       "   concave_points3  symmetry3  fractal_dimension3  Diagnosis  \n",
       "0         0.912027   0.598462            0.418864          0  \n",
       "1         0.639175   0.233590            0.222878          0  \n",
       "2         0.835052   0.403706            0.213433          0  \n",
       "3         0.884880   1.000000            0.773711          0  \n",
       "4         0.558419   0.157500            0.142595          0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (455,) (114, 30) (114,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and target\n",
    "\n",
    "X = data_cancer.drop('Diagnosis', axis=1)\n",
    "y = data_cancer['Diagnosis']\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled_data = data_cancer.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Define the split size for the training set\n",
    "train_size = int(0.8 * len(shuffled_data))  # 80% of data for training, 20% for testing\n",
    "\n",
    "# Split the data\n",
    "\n",
    "train_data = shuffled_data[:train_size]\n",
    "\n",
    "test_data = shuffled_data[train_size:]\n",
    "\n",
    "X_train = train_data.drop('Diagnosis', axis=1)  # Replace 'target_column' with your actual target column name\n",
    "y_train = train_data['Diagnosis']\n",
    "\n",
    "X_test = test_data.drop('Diagnosis', axis=1)\n",
    "y_test = test_data['Diagnosis']\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 7.337912164504649\n",
      "Epoch: 0, Loss: 0.3840079615928407\n",
      "Epoch: 1, Loss: 0.23560230931157594\n",
      "Epoch: 2, Loss: 0.19270902989392868\n",
      "Epoch: 3, Loss: 0.16774904530501744\n",
      "Epoch: 4, Loss: 0.1529132935911347\n",
      "Epoch: 5, Loss: 0.1396896876121964\n",
      "Epoch: 6, Loss: 0.1345099175815805\n",
      "Epoch: 7, Loss: 0.12632701220340892\n",
      "Epoch: 8, Loss: 0.11981109079743714\n",
      "Epoch: 9, Loss: 0.11464625681789599\n",
      "Epoch: 10, Loss: 0.11035843262107721\n",
      "Epoch: 11, Loss: 0.10525888794147266\n",
      "Epoch: 12, Loss: 0.10231822087776926\n",
      "Epoch: 13, Loss: 0.09900400819180254\n",
      "Epoch: 14, Loss: 0.09736798207690796\n",
      "Epoch: 15, Loss: 0.09258619678003471\n",
      "Epoch: 16, Loss: 0.09143076996083399\n",
      "Epoch: 17, Loss: 0.08861897854237955\n",
      "Epoch: 18, Loss: 0.08639871868505075\n",
      "Epoch: 19, Loss: 0.08427672372924422\n",
      "Epoch: 20, Loss: 0.08286287460494751\n",
      "Epoch: 21, Loss: 0.0808927342722591\n",
      "Epoch: 22, Loss: 0.07913041463887066\n",
      "Epoch: 23, Loss: 0.0786830514405196\n",
      "Epoch: 24, Loss: 0.0767752556805418\n",
      "Epoch: 25, Loss: 0.07569935564554502\n",
      "Epoch: 26, Loss: 0.07347432887632417\n",
      "Epoch: 27, Loss: 0.07360885082812962\n",
      "Stopping early due to increase in average loss.\n",
      "Test loss: 0.10354916384495072\n",
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# train the stochastic linear regression model\n",
    "\n",
    "w, b = initialize_weights(X_train.shape[1])\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "w, b, losses, training_time = train_sgd(X_train, y_train, w, b, alpha, lr, epochs)\n",
    "\n",
    "test_loss = test(X_test, y_test, w, b)\n",
    "\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# test accuracy\n",
    "\n",
    "y_pred = predict(X_test.to_numpy(), w, b)\n",
    "\n",
    "test_accuracy = np.mean(y_pred == y_test.to_numpy())\n",
    "\n",
    "print(f'Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
